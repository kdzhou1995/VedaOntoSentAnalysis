{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kdzhou\\.conda\\envs\\PyTorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import logging\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defines tokenizer to tokenize input.\n",
    "## All words unknown to the vocabulary will be split into subwords all the way down to invidual characters\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input sentence\n",
    "text = \"Im cool with my teacher so i was gonna ask for tht persons name but im too shy to do tht too \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'im', 'cool', 'with', 'my', 'teacher', 'so', 'i', 'was', 'gonna', 'ask', 'for', 'th', '##t', 'persons', 'name', 'but', 'im', 'too', 'shy', 'to', 'do', 'th', '##t', 'too', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "## adding BERT defined separators in front\n",
    "\n",
    "markedUpText = \"[CLS]\" + text + \"[SEP]\"\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(markedUpText)\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['##!',\n",
       " '##\"',\n",
       " '###',\n",
       " '##$',\n",
       " '##%',\n",
       " '##&',\n",
       " \"##'\",\n",
       " '##(',\n",
       " '##)',\n",
       " '##*',\n",
       " '##+',\n",
       " '##,',\n",
       " '##-',\n",
       " '##.',\n",
       " '##/',\n",
       " '##:',\n",
       " '##;',\n",
       " '##<',\n",
       " '##=',\n",
       " '##>',\n",
       " '##?',\n",
       " '##@',\n",
       " '##[',\n",
       " '##\\\\',\n",
       " '##]',\n",
       " '##^',\n",
       " '##_',\n",
       " '##`',\n",
       " '##{',\n",
       " '##|',\n",
       " '##}',\n",
       " '##~',\n",
       " '##¡',\n",
       " '##¢',\n",
       " '##£',\n",
       " '##¤',\n",
       " '##¥',\n",
       " '##¦',\n",
       " '##§',\n",
       " '##¨',\n",
       " '##©',\n",
       " '##ª',\n",
       " '##«',\n",
       " '##¬',\n",
       " '##®',\n",
       " '##±',\n",
       " '##´',\n",
       " '##µ',\n",
       " '##¶',\n",
       " '##·',\n",
       " '##º',\n",
       " '##»',\n",
       " '##¼',\n",
       " '##¾',\n",
       " '##¿',\n",
       " '##æ',\n",
       " '##ð',\n",
       " '##÷',\n",
       " '##þ',\n",
       " '##đ',\n",
       " '##ħ',\n",
       " '##ŋ',\n",
       " '##œ',\n",
       " '##ƒ',\n",
       " '##ɐ',\n",
       " '##ɑ',\n",
       " '##ɒ',\n",
       " '##ɔ',\n",
       " '##ɕ',\n",
       " '##ə',\n",
       " '##ɡ',\n",
       " '##ɣ',\n",
       " '##ɨ',\n",
       " '##ɪ',\n",
       " '##ɫ',\n",
       " '##ɬ',\n",
       " '##ɯ',\n",
       " '##ɲ',\n",
       " '##ɴ',\n",
       " '##ɹ',\n",
       " '##ɾ',\n",
       " '##ʀ',\n",
       " '##ʁ',\n",
       " '##ʂ',\n",
       " '##ʃ',\n",
       " '##ʉ',\n",
       " '##ʊ',\n",
       " '##ʋ',\n",
       " '##ʌ',\n",
       " '##ʎ',\n",
       " '##ʐ',\n",
       " '##ʑ',\n",
       " '##ʒ',\n",
       " '##ʔ',\n",
       " '##ʰ',\n",
       " '##ʲ',\n",
       " '##ʳ',\n",
       " '##ʷ',\n",
       " '##ʸ',\n",
       " '##ʻ',\n",
       " '##ʼ',\n",
       " '##ʾ',\n",
       " '##ʿ',\n",
       " '##ˈ',\n",
       " '##ˡ',\n",
       " '##ˢ',\n",
       " '##ˣ',\n",
       " '##ˤ',\n",
       " '##β',\n",
       " '##γ',\n",
       " '##δ',\n",
       " '##ε',\n",
       " '##ζ',\n",
       " '##θ',\n",
       " '##κ',\n",
       " '##λ',\n",
       " '##μ',\n",
       " '##ξ',\n",
       " '##ο',\n",
       " '##π',\n",
       " '##ρ',\n",
       " '##σ',\n",
       " '##τ',\n",
       " '##υ',\n",
       " '##φ',\n",
       " '##χ',\n",
       " '##ψ',\n",
       " '##ω',\n",
       " '##б',\n",
       " '##г',\n",
       " '##д',\n",
       " '##ж',\n",
       " '##з',\n",
       " '##м',\n",
       " '##п',\n",
       " '##с',\n",
       " '##у',\n",
       " '##ф',\n",
       " '##х',\n",
       " '##ц',\n",
       " '##ч',\n",
       " '##ш',\n",
       " '##щ',\n",
       " '##ъ',\n",
       " '##э',\n",
       " '##ю',\n",
       " '##ђ',\n",
       " '##є',\n",
       " '##і',\n",
       " '##ј',\n",
       " '##љ',\n",
       " '##њ',\n",
       " '##ћ',\n",
       " '##ӏ',\n",
       " '##ա',\n",
       " '##բ',\n",
       " '##գ',\n",
       " '##դ',\n",
       " '##ե',\n",
       " '##թ',\n",
       " '##ի',\n",
       " '##լ',\n",
       " '##կ',\n",
       " '##հ',\n",
       " '##մ',\n",
       " '##յ',\n",
       " '##ն',\n",
       " '##ո',\n",
       " '##պ',\n",
       " '##ս',\n",
       " '##վ',\n",
       " '##տ',\n",
       " '##ր',\n",
       " '##ւ',\n",
       " '##ք',\n",
       " '##־',\n",
       " '##א',\n",
       " '##ב',\n",
       " '##ג',\n",
       " '##ד',\n",
       " '##ו',\n",
       " '##ז',\n",
       " '##ח',\n",
       " '##ט',\n",
       " '##י',\n",
       " '##ך',\n",
       " '##כ',\n",
       " '##ל',\n",
       " '##ם',\n",
       " '##מ',\n",
       " '##ן',\n",
       " '##נ',\n",
       " '##ס',\n",
       " '##ע',\n",
       " '##ף',\n",
       " '##פ',\n",
       " '##ץ',\n",
       " '##צ',\n",
       " '##ק',\n",
       " '##ר',\n",
       " '##ש',\n",
       " '##ת',\n",
       " '##،',\n",
       " '##ء',\n",
       " '##ب',\n",
       " '##ت',\n",
       " '##ث',\n",
       " '##ج',\n",
       " '##ح',\n",
       " '##خ',\n",
       " '##ذ',\n",
       " '##ز',\n",
       " '##س',\n",
       " '##ش',\n",
       " '##ص',\n",
       " '##ض',\n",
       " '##ط',\n",
       " '##ظ',\n",
       " '##ع',\n",
       " '##غ',\n",
       " '##ـ',\n",
       " '##ف',\n",
       " '##ق',\n",
       " '##ك',\n",
       " '##و',\n",
       " '##ى',\n",
       " '##ٹ',\n",
       " '##پ',\n",
       " '##چ',\n",
       " '##ک',\n",
       " '##گ',\n",
       " '##ں',\n",
       " '##ھ',\n",
       " '##ہ',\n",
       " '##ے',\n",
       " '##अ',\n",
       " '##आ',\n",
       " '##उ',\n",
       " '##ए',\n",
       " '##क',\n",
       " '##ख',\n",
       " '##ग',\n",
       " '##च',\n",
       " '##ज',\n",
       " '##ट',\n",
       " '##ड',\n",
       " '##ण',\n",
       " '##त',\n",
       " '##थ',\n",
       " '##द',\n",
       " '##ध',\n",
       " '##न',\n",
       " '##प',\n",
       " '##ब',\n",
       " '##भ',\n",
       " '##म',\n",
       " '##य',\n",
       " '##र',\n",
       " '##ल',\n",
       " '##व',\n",
       " '##श',\n",
       " '##ष',\n",
       " '##स',\n",
       " '##ह',\n",
       " '##ा',\n",
       " '##ि',\n",
       " '##ी',\n",
       " '##ो',\n",
       " '##।',\n",
       " '##॥',\n",
       " '##ং',\n",
       " '##অ',\n",
       " '##আ',\n",
       " '##ই',\n",
       " '##উ',\n",
       " '##এ',\n",
       " '##ও',\n",
       " '##ক',\n",
       " '##খ',\n",
       " '##গ',\n",
       " '##চ',\n",
       " '##ছ',\n",
       " '##জ',\n",
       " '##ট',\n",
       " '##ড',\n",
       " '##ণ',\n",
       " '##ত',\n",
       " '##থ',\n",
       " '##দ',\n",
       " '##ধ',\n",
       " '##ন',\n",
       " '##প',\n",
       " '##ব',\n",
       " '##ভ',\n",
       " '##ম',\n",
       " '##য',\n",
       " '##র',\n",
       " '##ল',\n",
       " '##শ',\n",
       " '##ষ',\n",
       " '##স',\n",
       " '##হ',\n",
       " '##া',\n",
       " '##ি',\n",
       " '##ী',\n",
       " '##ে',\n",
       " '##க',\n",
       " '##ச',\n",
       " '##ட',\n",
       " '##த',\n",
       " '##ந',\n",
       " '##ன',\n",
       " '##ப',\n",
       " '##ம',\n",
       " '##ய',\n",
       " '##ர',\n",
       " '##ல',\n",
       " '##ள',\n",
       " '##வ',\n",
       " '##ா',\n",
       " '##ி',\n",
       " '##ு',\n",
       " '##ே',\n",
       " '##ை',\n",
       " '##ನ',\n",
       " '##ರ',\n",
       " '##ಾ',\n",
       " '##ක',\n",
       " '##ය',\n",
       " '##ර',\n",
       " '##ල',\n",
       " '##ව',\n",
       " '##ා',\n",
       " '##ก',\n",
       " '##ง',\n",
       " '##ต',\n",
       " '##ท',\n",
       " '##น',\n",
       " '##พ',\n",
       " '##ม',\n",
       " '##ย',\n",
       " '##ร',\n",
       " '##ล',\n",
       " '##ว',\n",
       " '##ส',\n",
       " '##อ',\n",
       " '##า',\n",
       " '##เ',\n",
       " '##་',\n",
       " '##།',\n",
       " '##ག',\n",
       " '##ང',\n",
       " '##ད',\n",
       " '##ན',\n",
       " '##པ',\n",
       " '##བ',\n",
       " '##མ',\n",
       " '##འ',\n",
       " '##ར',\n",
       " '##ལ',\n",
       " '##ས',\n",
       " '##မ',\n",
       " '##ა',\n",
       " '##ბ',\n",
       " '##გ',\n",
       " '##დ',\n",
       " '##ე',\n",
       " '##ვ',\n",
       " '##თ',\n",
       " '##ი',\n",
       " '##კ',\n",
       " '##ლ',\n",
       " '##მ',\n",
       " '##ნ',\n",
       " '##ო',\n",
       " '##რ',\n",
       " '##ს',\n",
       " '##ტ',\n",
       " '##უ',\n",
       " '##ᄀ',\n",
       " '##ᄂ',\n",
       " '##ᄃ',\n",
       " '##ᄅ',\n",
       " '##ᄆ',\n",
       " '##ᄇ',\n",
       " '##ᄉ',\n",
       " '##ᄊ',\n",
       " '##ᄋ',\n",
       " '##ᄌ',\n",
       " '##ᄎ',\n",
       " '##ᄏ',\n",
       " '##ᄐ',\n",
       " '##ᄑ',\n",
       " '##ᄒ',\n",
       " '##ᅡ',\n",
       " '##ᅢ',\n",
       " '##ᅥ',\n",
       " '##ᅦ',\n",
       " '##ᅧ',\n",
       " '##ᅩ',\n",
       " '##ᅪ',\n",
       " '##ᅭ',\n",
       " '##ᅮ',\n",
       " '##ᅯ',\n",
       " '##ᅲ',\n",
       " '##ᅳ',\n",
       " '##ᅴ',\n",
       " '##ᅵ',\n",
       " '##ᆨ',\n",
       " '##ᆫ',\n",
       " '##ᆯ',\n",
       " '##ᆷ',\n",
       " '##ᆸ',\n",
       " '##ᆼ',\n",
       " '##ᴬ',\n",
       " '##ᴮ',\n",
       " '##ᴰ',\n",
       " '##ᴵ',\n",
       " '##ᴺ',\n",
       " '##ᵀ',\n",
       " '##ᵃ',\n",
       " '##ᵇ',\n",
       " '##ᵈ',\n",
       " '##ᵉ',\n",
       " '##ᵍ',\n",
       " '##ᵏ',\n",
       " '##ᵐ',\n",
       " '##ᵒ',\n",
       " '##ᵖ',\n",
       " '##ᵗ',\n",
       " '##ᵘ',\n",
       " '##ᵣ',\n",
       " '##ᵤ',\n",
       " '##ᵥ',\n",
       " '##ᶜ',\n",
       " '##ᶠ',\n",
       " '##‐',\n",
       " '##‑',\n",
       " '##‒',\n",
       " '##–',\n",
       " '##—',\n",
       " '##―',\n",
       " '##‖',\n",
       " '##‘',\n",
       " '##’',\n",
       " '##‚',\n",
       " '##“',\n",
       " '##”',\n",
       " '##„',\n",
       " '##†',\n",
       " '##‡',\n",
       " '##•',\n",
       " '##…',\n",
       " '##‰',\n",
       " '##′',\n",
       " '##″',\n",
       " '##›',\n",
       " '##‿',\n",
       " '##⁄',\n",
       " '##⁰',\n",
       " '##ⁱ',\n",
       " '##⁴',\n",
       " '##⁵',\n",
       " '##⁶',\n",
       " '##⁷',\n",
       " '##⁸',\n",
       " '##⁹',\n",
       " '##⁻',\n",
       " '##ⁿ',\n",
       " '##₅',\n",
       " '##₆',\n",
       " '##₇',\n",
       " '##₈',\n",
       " '##₉',\n",
       " '##₊',\n",
       " '##₍',\n",
       " '##₎',\n",
       " '##ₐ',\n",
       " '##ₑ',\n",
       " '##ₒ',\n",
       " '##ₓ',\n",
       " '##ₕ',\n",
       " '##ₖ',\n",
       " '##ₗ',\n",
       " '##ₘ',\n",
       " '##ₚ',\n",
       " '##ₛ',\n",
       " '##ₜ',\n",
       " '##₤',\n",
       " '##₩',\n",
       " '##€',\n",
       " '##₱',\n",
       " '##₹',\n",
       " '##ℓ',\n",
       " '##№',\n",
       " '##ℝ',\n",
       " '##™',\n",
       " '##⅓',\n",
       " '##⅔',\n",
       " '##←',\n",
       " '##↑',\n",
       " '##→',\n",
       " '##↓',\n",
       " '##↔',\n",
       " '##↦',\n",
       " '##⇄',\n",
       " '##⇌',\n",
       " '##⇒',\n",
       " '##∂',\n",
       " '##∅',\n",
       " '##∆',\n",
       " '##∇',\n",
       " '##∈',\n",
       " '##∗',\n",
       " '##∘',\n",
       " '##√',\n",
       " '##∞',\n",
       " '##∧',\n",
       " '##∨',\n",
       " '##∩',\n",
       " '##∪',\n",
       " '##≈',\n",
       " '##≡',\n",
       " '##≤',\n",
       " '##≥',\n",
       " '##⊂',\n",
       " '##⊆',\n",
       " '##⊕',\n",
       " '##⊗',\n",
       " '##⋅',\n",
       " '##─',\n",
       " '##│',\n",
       " '##■',\n",
       " '##▪',\n",
       " '##●',\n",
       " '##★',\n",
       " '##☆',\n",
       " '##☉',\n",
       " '##♠',\n",
       " '##♣',\n",
       " '##♥',\n",
       " '##♦',\n",
       " '##♯',\n",
       " '##⟨',\n",
       " '##⟩',\n",
       " '##ⱼ',\n",
       " '##⺩',\n",
       " '##⺼',\n",
       " '##⽥',\n",
       " '##、',\n",
       " '##。',\n",
       " '##〈',\n",
       " '##〉',\n",
       " '##《',\n",
       " '##》',\n",
       " '##「',\n",
       " '##」',\n",
       " '##『',\n",
       " '##』',\n",
       " '##〜',\n",
       " '##あ',\n",
       " '##い',\n",
       " '##う',\n",
       " '##え',\n",
       " '##お',\n",
       " '##か',\n",
       " '##き',\n",
       " '##く',\n",
       " '##け',\n",
       " '##こ',\n",
       " '##さ',\n",
       " '##し',\n",
       " '##す',\n",
       " '##せ',\n",
       " '##そ',\n",
       " '##た',\n",
       " '##ち',\n",
       " '##っ',\n",
       " '##つ',\n",
       " '##て',\n",
       " '##と',\n",
       " '##な',\n",
       " '##に',\n",
       " '##ぬ',\n",
       " '##ね',\n",
       " '##の',\n",
       " '##は',\n",
       " '##ひ',\n",
       " '##ふ',\n",
       " '##へ',\n",
       " '##ほ',\n",
       " '##ま',\n",
       " '##み',\n",
       " '##む',\n",
       " '##め',\n",
       " '##も',\n",
       " '##や',\n",
       " '##ゆ',\n",
       " '##よ',\n",
       " '##ら',\n",
       " '##り',\n",
       " '##る',\n",
       " '##れ',\n",
       " '##ろ',\n",
       " '##を',\n",
       " '##ん',\n",
       " '##ァ',\n",
       " '##ア',\n",
       " '##ィ',\n",
       " '##イ',\n",
       " '##ウ',\n",
       " '##ェ',\n",
       " '##エ',\n",
       " '##オ',\n",
       " '##カ',\n",
       " '##キ',\n",
       " '##ク',\n",
       " '##ケ',\n",
       " '##コ',\n",
       " '##サ',\n",
       " '##シ',\n",
       " '##ス',\n",
       " '##セ',\n",
       " '##タ',\n",
       " '##チ',\n",
       " '##ッ',\n",
       " '##ツ',\n",
       " '##テ',\n",
       " '##ト',\n",
       " '##ナ',\n",
       " '##ニ',\n",
       " '##ノ',\n",
       " '##ハ',\n",
       " '##ヒ',\n",
       " '##フ',\n",
       " '##ヘ',\n",
       " '##ホ',\n",
       " '##マ',\n",
       " '##ミ',\n",
       " '##ム',\n",
       " '##メ',\n",
       " '##モ',\n",
       " '##ャ',\n",
       " '##ュ',\n",
       " '##ョ',\n",
       " '##ラ',\n",
       " '##リ',\n",
       " '##ル',\n",
       " '##レ',\n",
       " '##ロ',\n",
       " '##ワ',\n",
       " '##ン',\n",
       " '##・',\n",
       " '##ー',\n",
       " '##一',\n",
       " '##三',\n",
       " '##上',\n",
       " '##下',\n",
       " '##不',\n",
       " '##世',\n",
       " '##中',\n",
       " '##主',\n",
       " '##久',\n",
       " '##之',\n",
       " '##也',\n",
       " '##事',\n",
       " '##二',\n",
       " '##五',\n",
       " '##井',\n",
       " '##京',\n",
       " '##人',\n",
       " '##亻',\n",
       " '##仁',\n",
       " '##介',\n",
       " '##代',\n",
       " '##仮',\n",
       " '##伊',\n",
       " '##会',\n",
       " '##佐',\n",
       " '##侍',\n",
       " '##保',\n",
       " '##信',\n",
       " '##健',\n",
       " '##元',\n",
       " '##光',\n",
       " '##八',\n",
       " '##公',\n",
       " '##内',\n",
       " '##出',\n",
       " '##分',\n",
       " '##前',\n",
       " '##劉',\n",
       " '##力',\n",
       " '##加',\n",
       " '##勝',\n",
       " '##北',\n",
       " '##区',\n",
       " '##十',\n",
       " '##千',\n",
       " '##南',\n",
       " '##博',\n",
       " '##原',\n",
       " '##口',\n",
       " '##古',\n",
       " '##史',\n",
       " '##司',\n",
       " '##合',\n",
       " '##吉',\n",
       " '##同',\n",
       " '##名',\n",
       " '##和',\n",
       " '##囗',\n",
       " '##四',\n",
       " '##国',\n",
       " '##國',\n",
       " '##土',\n",
       " '##地',\n",
       " '##坂',\n",
       " '##城',\n",
       " '##堂',\n",
       " '##場',\n",
       " '##士',\n",
       " '##夏',\n",
       " '##外',\n",
       " '##大',\n",
       " '##天',\n",
       " '##太',\n",
       " '##夫',\n",
       " '##奈',\n",
       " '##女',\n",
       " '##子',\n",
       " '##学',\n",
       " '##宀',\n",
       " '##宇',\n",
       " '##安',\n",
       " '##宗',\n",
       " '##定',\n",
       " '##宣',\n",
       " '##宮',\n",
       " '##家',\n",
       " '##宿',\n",
       " '##寺',\n",
       " '##將',\n",
       " '##小',\n",
       " '##尚',\n",
       " '##山',\n",
       " '##岡',\n",
       " '##島',\n",
       " '##崎',\n",
       " '##川',\n",
       " '##州',\n",
       " '##巿',\n",
       " '##帝',\n",
       " '##平',\n",
       " '##年',\n",
       " '##幸',\n",
       " '##广',\n",
       " '##弘',\n",
       " '##張',\n",
       " '##彳',\n",
       " '##後',\n",
       " '##御',\n",
       " '##德',\n",
       " '##心',\n",
       " '##忄',\n",
       " '##志',\n",
       " '##忠',\n",
       " '##愛',\n",
       " '##成',\n",
       " '##我',\n",
       " '##戦',\n",
       " '##戸',\n",
       " '##手',\n",
       " '##扌',\n",
       " '##政',\n",
       " '##文',\n",
       " '##新',\n",
       " '##方',\n",
       " '##日',\n",
       " '##明',\n",
       " '##星',\n",
       " '##春',\n",
       " '##昭',\n",
       " '##智',\n",
       " '##曲',\n",
       " '##書',\n",
       " '##月',\n",
       " '##有',\n",
       " '##朝',\n",
       " '##木',\n",
       " '##本',\n",
       " '##李',\n",
       " '##村',\n",
       " '##東',\n",
       " '##松',\n",
       " '##林',\n",
       " '##森',\n",
       " '##楊',\n",
       " '##樹',\n",
       " '##橋',\n",
       " '##歌',\n",
       " '##止',\n",
       " '##正',\n",
       " '##武',\n",
       " '##比',\n",
       " '##氏',\n",
       " '##民',\n",
       " '##水',\n",
       " '##氵',\n",
       " '##氷',\n",
       " '##永',\n",
       " '##江',\n",
       " '##沢',\n",
       " '##河',\n",
       " '##治',\n",
       " '##法',\n",
       " '##海',\n",
       " '##清',\n",
       " '##漢',\n",
       " '##瀬',\n",
       " '##火',\n",
       " '##版',\n",
       " '##犬',\n",
       " '##王',\n",
       " '##生',\n",
       " '##田',\n",
       " '##男',\n",
       " '##疒',\n",
       " '##発',\n",
       " '##白',\n",
       " '##的',\n",
       " '##皇',\n",
       " '##目',\n",
       " '##相',\n",
       " '##省',\n",
       " '##真',\n",
       " '##石',\n",
       " '##示',\n",
       " '##社',\n",
       " '##神',\n",
       " '##福',\n",
       " '##禾',\n",
       " '##秀',\n",
       " '##秋',\n",
       " '##空',\n",
       " '##立',\n",
       " '##章',\n",
       " '##竹',\n",
       " '##糹',\n",
       " '##美',\n",
       " '##義',\n",
       " '##耳',\n",
       " '##良',\n",
       " '##艹',\n",
       " '##花',\n",
       " '##英',\n",
       " '##華',\n",
       " '##葉',\n",
       " '##藤',\n",
       " '##行',\n",
       " '##街',\n",
       " '##西',\n",
       " '##見',\n",
       " '##訁',\n",
       " '##語',\n",
       " '##谷',\n",
       " '##貝',\n",
       " '##貴',\n",
       " '##車',\n",
       " '##軍',\n",
       " '##辶',\n",
       " '##道',\n",
       " '##郎',\n",
       " '##郡',\n",
       " '##部',\n",
       " '##都',\n",
       " '##里',\n",
       " '##野',\n",
       " '##金',\n",
       " '##鈴',\n",
       " '##镇',\n",
       " '##長',\n",
       " '##門',\n",
       " '##間',\n",
       " '##阝',\n",
       " '##阿',\n",
       " '##陳',\n",
       " '##陽',\n",
       " '##雄',\n",
       " '##青',\n",
       " '##面',\n",
       " '##風',\n",
       " '##食',\n",
       " '##香',\n",
       " '##馬',\n",
       " '##高',\n",
       " '##龍',\n",
       " '##龸',\n",
       " '##ﬁ',\n",
       " '##ﬂ',\n",
       " '##！',\n",
       " '##（',\n",
       " '##）',\n",
       " '##，',\n",
       " '##－',\n",
       " '##．',\n",
       " '##／',\n",
       " '##：',\n",
       " '##？',\n",
       " '##～']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## exploring the vocabulary of the BERT tokenizer\n",
    "\n",
    "## [0] - [PAD]\n",
    "## [100] - [UNK]\n",
    "## [101] - [CLS]\n",
    "## [102] - [SEP]\n",
    "## [104] - [MASK]\n",
    "## [999:1996] are starting characters ! ... ~\n",
    "## [1996:29612] are words\n",
    "## [29612:] are subwords\n",
    "\n",
    "\n",
    "list(tokenizer.vocab.keys())[29612:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 101\n",
      "im 10047\n",
      "cool 4658\n",
      "with 2007\n",
      "my 2026\n",
      "teacher 3836\n",
      "so 2061\n",
      "i 1045\n",
      "was 2001\n",
      "gonna 6069\n",
      "ask 3198\n",
      "for 2005\n",
      "th 16215\n",
      "##t 2102\n",
      "persons 5381\n",
      "name 2171\n",
      "but 2021\n",
      "im 10047\n",
      "too 2205\n",
      "shy 11004\n",
      "to 2000\n",
      "do 2079\n",
      "th 16215\n",
      "##t 2102\n",
      "too 2205\n",
      "[SEP] 102\n"
     ]
    }
   ],
   "source": [
    "## map words to indices\n",
    "\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "for token, id in zip(tokenized_text, token_ids):\n",
    "    print(token, id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 101 1\n",
      "im 10047 1\n",
      "cool 4658 1\n",
      "with 2007 1\n",
      "my 2026 1\n",
      "teacher 3836 1\n",
      "so 2061 1\n",
      "i 1045 1\n",
      "was 2001 1\n",
      "gonna 6069 1\n",
      "ask 3198 1\n",
      "for 2005 1\n",
      "th 16215 1\n",
      "##t 2102 1\n",
      "persons 5381 1\n",
      "name 2171 1\n",
      "but 2021 1\n",
      "im 10047 1\n",
      "too 2205 1\n",
      "shy 11004 1\n",
      "to 2000 1\n",
      "do 2079 1\n",
      "th 16215 1\n",
      "##t 2102 1\n",
      "too 2205 1\n",
      "[SEP] 102 1\n"
     ]
    }
   ],
   "source": [
    "## add sentence ids to each token\n",
    "\n",
    "sentence_ids = [1] * len(token_ids)\n",
    "\n",
    "for token, token_id, sentence_id in zip(tokenized_text, token_ids, sentence_ids):\n",
    "    print(token, token_id, sentence_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101, 10047,  4658,  2007,  2026,  3836,  2061,  1045,  2001,  6069,\n",
      "          3198,  2005, 16215,  2102,  5381,  2171,  2021, 10047,  2205, 11004,\n",
      "          2000,  2079, 16215,  2102,  2205,   102]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]])\n"
     ]
    }
   ],
   "source": [
    "## We need to turn token id and segment id lists into tensors\n",
    "\n",
    "tokens_tensor = torch.tensor([token_ids])\n",
    "\n",
    "segment_tensor = torch.tensor([sentence_ids])\n",
    "\n",
    "print(tokens_tensor)\n",
    "\n",
    "print(segment_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kdzhou\\.conda\\envs\\PyTorch\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kdzhou\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load the BERT pretrained model\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0 shape: torch.Size([1, 26, 768])\n",
      "\t length of a sentence is 26\n",
      "\t\t[CLS]: tensor([ 0.1686, -0.2858, -0.3261])\n",
      "\t\tim: tensor([-0.1992, -0.2818, -0.4229])\n",
      "\t\tcool: tensor([0.2039, 0.0116, 0.3376])\n",
      "\t\twith: tensor([-0.5715,  0.1363,  0.2100])\n",
      "\t\tmy: tensor([ 0.4374,  0.5032, -0.5967])\n",
      "\t\tteacher: tensor([-0.1058, -0.3168, -0.1371])\n",
      "\t\tso: tensor([-0.1433,  0.1276,  0.0729])\n",
      "\t\ti: tensor([-0.1160,  0.2462, -0.0892])\n",
      "\t\twas: tensor([-0.2475, -0.7300,  0.2477])\n",
      "\t\tgonna: tensor([ 0.8068,  0.0414, -0.0809])\n",
      "\t\task: tensor([-1.9051, -1.2284, -0.0197])\n",
      "\t\tfor: tensor([-0.1977,  0.5124, -0.7430])\n",
      "\t\tth: tensor([ 0.0146, -0.9605,  1.1715])\n",
      "\t\t##t: tensor([-1.3675,  0.3061,  0.9769])\n",
      "\t\tpersons: tensor([-0.8862,  0.7206, -0.3123])\n",
      "\t\tname: tensor([ 0.2807,  0.8009, -0.3979])\n",
      "\t\tbut: tensor([-0.0458,  0.7050,  0.1048])\n",
      "\t\tim: tensor([-0.4673, -0.1840, -0.1914])\n",
      "\t\ttoo: tensor([-0.5517,  0.5391, -0.3514])\n",
      "\t\tshy: tensor([-1.0193, -0.0746, -0.0489])\n",
      "\t\tto: tensor([ 0.5639,  0.5797, -0.2717])\n",
      "\t\tdo: tensor([ 0.4234, -0.3302,  0.2042])\n",
      "\t\tth: tensor([ 0.3789, -1.0169,  0.9784])\n",
      "\t\t##t: tensor([-1.0867,  0.4266,  0.8276])\n",
      "\t\ttoo: tensor([-0.5751,  0.6804, -0.6275])\n",
      "\t\t[SEP]: tensor([-0.3503,  0.2602, -0.1015])\n",
      "layer: 1 shape: torch.Size([1, 26, 768])\n",
      "\t length of a sentence is 26\n",
      "\t\t[CLS]: tensor([ 0.1094,  0.0691, -0.1555])\n",
      "\t\tim: tensor([ 0.4228, -0.3384, -0.7487])\n",
      "\t\tcool: tensor([ 0.5086, -0.2861,  0.7704])\n",
      "\t\twith: tensor([-0.3386,  0.4894,  0.3190])\n",
      "\t\tmy: tensor([ 0.5751,  0.5669, -0.4442])\n",
      "\t\tteacher: tensor([ 0.6605, -0.0633, -1.0231])\n",
      "\t\tso: tensor([0.0654, 0.1032, 0.1352])\n",
      "\t\ti: tensor([ 0.2127,  0.5045, -0.4080])\n",
      "\t\twas: tensor([ 0.0515, -1.0554,  0.4233])\n",
      "\t\tgonna: tensor([ 1.1435, -0.0394,  0.2809])\n",
      "\t\task: tensor([-1.5675, -1.3093,  0.6279])\n",
      "\t\tfor: tensor([ 0.2380,  0.4808, -0.7178])\n",
      "\t\tth: tensor([ 0.3201, -1.2992,  1.7003])\n",
      "\t\t##t: tensor([-1.3184,  0.1688,  1.0820])\n",
      "\t\tpersons: tensor([-0.7604,  0.9265,  0.0121])\n",
      "\t\tname: tensor([ 0.5252,  1.1065, -0.4677])\n",
      "\t\tbut: tensor([0.2253, 0.9024, 0.0343])\n",
      "\t\tim: tensor([ 0.0124, -0.4009, -0.5658])\n",
      "\t\ttoo: tensor([-0.2324,  0.7219, -0.5009])\n",
      "\t\tshy: tensor([-0.3280, -0.2636, -0.1320])\n",
      "\t\tto: tensor([ 0.9364,  0.4722, -0.1158])\n",
      "\t\tdo: tensor([ 0.3160, -0.1541, -0.5111])\n",
      "\t\tth: tensor([ 0.7045, -1.2626,  1.2818])\n",
      "\t\t##t: tensor([-0.9980,  0.2024,  0.8685])\n",
      "\t\ttoo: tensor([-0.1842,  0.7987, -0.9817])\n",
      "\t\t[SEP]: tensor([-0.0998,  0.3030, -0.3273])\n",
      "layer: 2 shape: torch.Size([1, 26, 768])\n",
      "\t length of a sentence is 26\n",
      "\t\t[CLS]: tensor([-0.0269, -0.0890, -0.2918])\n",
      "\t\tim: tensor([ 0.1319, -0.2169, -0.2957])\n",
      "\t\tcool: tensor([ 0.7675, -0.3752,  0.9202])\n",
      "\t\twith: tensor([-0.2535,  0.8886,  0.3656])\n",
      "\t\tmy: tensor([ 0.4759,  0.2777, -0.4463])\n",
      "\t\tteacher: tensor([ 0.6013,  0.0404, -0.7748])\n",
      "\t\tso: tensor([ 0.4961, -0.0902,  0.2938])\n",
      "\t\ti: tensor([ 0.5361,  0.3408, -0.5145])\n",
      "\t\twas: tensor([ 0.2402, -0.5021,  0.4026])\n",
      "\t\tgonna: tensor([ 1.4364, -0.5237,  0.6388])\n",
      "\t\task: tensor([-1.5427, -0.6929,  1.0687])\n",
      "\t\tfor: tensor([ 0.3702,  0.3494, -0.3247])\n",
      "\t\tth: tensor([ 0.4333, -0.7395,  1.9573])\n",
      "\t\t##t: tensor([-1.2585,  0.3420,  0.9071])\n",
      "\t\tpersons: tensor([-0.6805,  1.0753,  0.3096])\n",
      "\t\tname: tensor([ 0.5794,  1.0433, -0.3118])\n",
      "\t\tbut: tensor([0.2988, 1.0092, 0.5348])\n",
      "\t\tim: tensor([-0.2761, -0.1082, -0.2069])\n",
      "\t\ttoo: tensor([0.3987, 1.1553, 0.2145])\n",
      "\t\tshy: tensor([-0.7094, -0.1372,  0.2847])\n",
      "\t\tto: tensor([1.0746, 0.3613, 0.2297])\n",
      "\t\tdo: tensor([ 0.3961, -0.0571, -0.3218])\n",
      "\t\tth: tensor([ 0.8676, -0.6608,  1.3042])\n",
      "\t\t##t: tensor([-0.9704,  0.4394,  0.5521])\n",
      "\t\ttoo: tensor([ 0.1684,  1.1368, -0.3591])\n",
      "\t\t[SEP]: tensor([-0.0984,  0.1717, -0.0575])\n",
      "layer: 3 shape: torch.Size([1, 26, 768])\n",
      "\t length of a sentence is 26\n",
      "\t\t[CLS]: tensor([ 0.0615, -0.1970, -0.1389])\n",
      "\t\tim: tensor([ 0.7079,  0.7100, -0.2005])\n",
      "\t\tcool: tensor([ 0.9948, -0.8469,  0.8588])\n",
      "\t\twith: tensor([-0.2988,  1.0464,  0.5956])\n",
      "\t\tmy: tensor([ 0.2910,  0.1701, -0.5048])\n",
      "\t\tteacher: tensor([ 0.8474, -0.2338, -0.4377])\n",
      "\t\tso: tensor([ 0.4708, -0.3890,  0.3416])\n",
      "\t\ti: tensor([ 0.2741,  0.3113, -0.1469])\n",
      "\t\twas: tensor([ 0.2810, -0.4506,  0.5004])\n",
      "\t\tgonna: tensor([ 1.4030, -0.4805,  0.2412])\n",
      "\t\task: tensor([-1.3132, -0.2461,  1.0783])\n",
      "\t\tfor: tensor([ 0.3470,  0.3352, -0.2884])\n",
      "\t\tth: tensor([ 0.5189, -1.0391,  0.9628])\n",
      "\t\t##t: tensor([-1.1442,  0.2258,  1.1307])\n",
      "\t\tpersons: tensor([-0.8830,  1.3406,  0.2668])\n",
      "\t\tname: tensor([ 0.8633,  1.2437, -0.1272])\n",
      "\t\tbut: tensor([0.3275, 0.5771, 0.4753])\n",
      "\t\tim: tensor([ 0.2398,  0.7523, -0.5077])\n",
      "\t\ttoo: tensor([0.5263, 0.9017, 0.5886])\n",
      "\t\tshy: tensor([-0.6614,  0.1959,  0.3182])\n",
      "\t\tto: tensor([1.2047, 0.5634, 0.3151])\n",
      "\t\tdo: tensor([ 0.6186,  0.2837, -0.2930])\n",
      "\t\tth: tensor([ 0.9099, -0.8634,  0.6185])\n",
      "\t\t##t: tensor([-0.6071,  0.4121,  0.4663])\n",
      "\t\ttoo: tensor([ 0.7888,  0.8693, -0.4721])\n",
      "\t\t[SEP]: tensor([-0.0446, -0.0638,  0.0744])\n",
      "layer: 4 shape: torch.Size([1, 26, 768])\n",
      "\t length of a sentence is 26\n",
      "\t\t[CLS]: tensor([ 0.3093, -0.4587, -0.3418])\n",
      "\t\tim: tensor([0.9073, 0.9099, 0.4050])\n",
      "\t\tcool: tensor([ 1.4081, -0.2761,  0.9256])\n",
      "\t\twith: tensor([-0.1088,  0.7738,  0.7050])\n",
      "\t\tmy: tensor([ 0.6601,  0.3120, -0.5668])\n",
      "\t\tteacher: tensor([ 1.1766, -0.4022, -0.1672])\n",
      "\t\tso: tensor([ 0.3995, -0.6512,  0.0474])\n",
      "\t\ti: tensor([ 0.3177,  0.3689, -0.0763])\n",
      "\t\twas: tensor([-0.0010, -0.4412,  0.4329])\n",
      "\t\tgonna: tensor([ 1.4985, -0.5490,  0.3647])\n",
      "\t\task: tensor([-0.7418, -0.6423,  0.6637])\n",
      "\t\tfor: tensor([-0.3094, -0.0233, -0.1748])\n",
      "\t\tth: tensor([-0.0645, -0.5471,  0.4079])\n",
      "\t\t##t: tensor([-0.9187,  0.4997,  1.2317])\n",
      "\t\tpersons: tensor([-0.9554,  1.3878,  0.3419])\n",
      "\t\tname: tensor([ 1.1430,  0.8718, -0.2317])\n",
      "\t\tbut: tensor([-0.1631,  0.0756,  0.9349])\n",
      "\t\tim: tensor([0.3737, 1.0405, 0.2516])\n",
      "\t\ttoo: tensor([0.6088, 0.9048, 0.7369])\n",
      "\t\tshy: tensor([-0.7531,  0.2180,  0.5266])\n",
      "\t\tto: tensor([1.6128, 0.6045, 0.5478])\n",
      "\t\tdo: tensor([ 0.4333,  0.4185, -0.2712])\n",
      "\t\tth: tensor([ 3.3651e-01, -2.4845e-01, -1.4262e-04])\n",
      "\t\t##t: tensor([-0.3640,  0.5708,  0.6449])\n",
      "\t\ttoo: tensor([ 0.7235,  0.4232, -0.6813])\n",
      "\t\t[SEP]: tensor([-0.0131, -0.0473,  0.0187])\n",
      "layer: 5 shape: torch.Size([1, 26, 768])\n",
      "\t length of a sentence is 26\n",
      "\t\t[CLS]: tensor([ 0.2052, -0.4078, -0.1116])\n",
      "\t\tim: tensor([0.3173, 0.8590, 0.4987])\n",
      "\t\tcool: tensor([ 1.4539, -0.3810,  0.4817])\n",
      "\t\twith: tensor([0.2349, 0.7669, 0.1745])\n",
      "\t\tmy: tensor([ 0.7094,  0.2419, -0.2624])\n",
      "\t\tteacher: tensor([ 1.0896, -0.1374, -0.3684])\n",
      "\t\tso: tensor([ 0.3896, -0.7583, -0.1522])\n",
      "\t\ti: tensor([ 0.4791,  0.6103, -0.2300])\n",
      "\t\twas: tensor([ 0.1466, -0.1609,  0.3694])\n",
      "\t\tgonna: tensor([ 1.5707, -0.2176, -0.1407])\n",
      "\t\task: tensor([-0.6854, -0.1502,  0.4048])\n",
      "\t\tfor: tensor([-0.4104, -0.0957, -0.2992])\n",
      "\t\tth: tensor([-0.3921, -0.0096,  0.4125])\n",
      "\t\t##t: tensor([-0.8447,  0.5843,  0.5739])\n",
      "\t\tpersons: tensor([-0.8577,  1.3379,  0.2306])\n",
      "\t\tname: tensor([1.4880, 1.0805, 0.0336])\n",
      "\t\tbut: tensor([-0.2533,  0.3066,  0.5910])\n",
      "\t\tim: tensor([0.0229, 1.0265, 0.4895])\n",
      "\t\ttoo: tensor([0.3852, 0.4674, 0.7991])\n",
      "\t\tshy: tensor([-0.7342,  0.3529,  0.5645])\n",
      "\t\tto: tensor([1.5690, 0.7184, 0.1020])\n",
      "\t\tdo: tensor([ 0.5360,  0.5951, -0.1647])\n",
      "\t\tth: tensor([-0.0991,  0.2692,  0.3735])\n",
      "\t\t##t: tensor([-0.1373,  0.8486,  0.5521])\n",
      "\t\ttoo: tensor([ 0.5337,  0.4902, -0.3799])\n",
      "\t\t[SEP]: tensor([-0.0135, -0.0401,  0.0285])\n",
      "layer: 6 shape: torch.Size([1, 26, 768])\n",
      "\t length of a sentence is 26\n",
      "\t\t[CLS]: tensor([ 0.3892, -0.6006, -0.2032])\n",
      "\t\tim: tensor([-0.0413,  0.9427,  0.2345])\n",
      "\t\tcool: tensor([ 1.0048, -0.6085,  0.2352])\n",
      "\t\twith: tensor([ 0.2826, -0.0274, -0.0114])\n",
      "\t\tmy: tensor([ 0.7521,  0.1302, -0.4358])\n",
      "\t\tteacher: tensor([ 0.8047, -0.0249, -0.4522])\n",
      "\t\tso: tensor([ 0.6922, -0.9105, -0.1146])\n",
      "\t\ti: tensor([ 0.5545,  0.7124, -0.5121])\n",
      "\t\twas: tensor([-0.1294, -0.1425, -0.1064])\n",
      "\t\tgonna: tensor([ 1.2879,  0.0081, -0.4134])\n",
      "\t\task: tensor([-0.3916, -0.1755,  0.6395])\n",
      "\t\tfor: tensor([-0.6533, -0.6014, -0.2829])\n",
      "\t\tth: tensor([-0.7204,  0.0355,  0.2929])\n",
      "\t\t##t: tensor([-0.9839,  0.3553,  0.4622])\n",
      "\t\tpersons: tensor([-0.6105,  1.1769,  0.2358])\n",
      "\t\tname: tensor([ 1.3633,  0.9340, -0.3755])\n",
      "\t\tbut: tensor([-0.6968,  0.4978,  0.4107])\n",
      "\t\tim: tensor([-0.4682,  1.0185,  0.2382])\n",
      "\t\ttoo: tensor([0.1092, 0.0054, 0.8710])\n",
      "\t\tshy: tensor([-0.9056,  0.5314,  0.7452])\n",
      "\t\tto: tensor([1.3443, 0.5935, 0.1560])\n",
      "\t\tdo: tensor([ 0.6204,  0.1615, -0.2571])\n",
      "\t\tth: tensor([-0.2431,  0.1734,  0.0454])\n",
      "\t\t##t: tensor([-0.4893,  0.4166,  0.2443])\n",
      "\t\ttoo: tensor([ 0.3005,  0.1292, -0.4984])\n",
      "\t\t[SEP]: tensor([ 0.0179, -0.0394, -0.0169])\n",
      "layer: 7 shape: torch.Size([1, 26, 768])\n",
      "\t length of a sentence is 26\n",
      "\t\t[CLS]: tensor([ 0.3868, -0.3117, -0.0100])\n",
      "\t\tim: tensor([0.0860, 1.3178, 1.0520])\n",
      "\t\tcool: tensor([ 0.8303, -0.4073,  0.2465])\n",
      "\t\twith: tensor([0.1633, 0.1469, 0.0800])\n",
      "\t\tmy: tensor([ 0.7801, -0.1201, -0.6053])\n",
      "\t\tteacher: tensor([ 0.7090,  0.1996, -0.3669])\n",
      "\t\tso: tensor([ 0.1543, -0.4263,  0.3336])\n",
      "\t\ti: tensor([ 0.3353,  0.8183, -0.3478])\n",
      "\t\twas: tensor([-0.1495,  0.1838,  0.0923])\n",
      "\t\tgonna: tensor([ 1.2406,  0.1947, -0.1257])\n",
      "\t\task: tensor([-0.3278,  0.1332,  0.5807])\n",
      "\t\tfor: tensor([-0.3851, -0.6764, -0.2670])\n",
      "\t\tth: tensor([-0.8302,  0.0282,  0.5850])\n",
      "\t\t##t: tensor([-1.0402,  0.0665,  0.7490])\n",
      "\t\tpersons: tensor([-0.3658,  1.0136,  0.1548])\n",
      "\t\tname: tensor([ 1.6094,  1.2692, -0.2641])\n",
      "\t\tbut: tensor([-0.3709,  0.5076,  0.9106])\n",
      "\t\tim: tensor([-0.3959,  1.2141,  1.0140])\n",
      "\t\ttoo: tensor([0.1973, 0.3315, 1.2035])\n",
      "\t\tshy: tensor([-1.0659,  0.7746,  0.5543])\n",
      "\t\tto: tensor([ 0.4494, -0.0055,  0.6048])\n",
      "\t\tdo: tensor([ 0.9869,  0.4243, -0.0512])\n",
      "\t\tth: tensor([-0.4663,  0.3536,  0.3921])\n",
      "\t\t##t: tensor([-0.6238,  0.2224,  0.4861])\n",
      "\t\ttoo: tensor([ 0.2163, -0.3721, -1.2247])\n",
      "\t\t[SEP]: tensor([ 0.0095, -0.0008, -0.0054])\n",
      "layer: 8 shape: torch.Size([1, 26, 768])\n",
      "\t length of a sentence is 26\n",
      "\t\t[CLS]: tensor([ 0.3432, -0.1803, -0.8442])\n",
      "\t\tim: tensor([0.1275, 1.2107, 0.4548])\n",
      "\t\tcool: tensor([ 0.7097, -0.4811,  0.2636])\n",
      "\t\twith: tensor([ 0.1178, -0.5377,  0.0259])\n",
      "\t\tmy: tensor([ 0.8335,  0.2129, -0.9903])\n",
      "\t\tteacher: tensor([ 0.5852,  0.2833, -0.3556])\n",
      "\t\tso: tensor([-0.1438, -0.0596,  0.9330])\n",
      "\t\ti: tensor([ 0.2184,  0.6882, -0.8186])\n",
      "\t\twas: tensor([ 0.0054,  0.2178, -0.9252])\n",
      "\t\tgonna: tensor([ 1.3780,  0.2825, -0.3350])\n",
      "\t\task: tensor([-0.1271,  0.3333,  0.3300])\n",
      "\t\tfor: tensor([-0.1403, -0.1952, -0.5323])\n",
      "\t\tth: tensor([-0.6039,  0.0683,  0.5690])\n",
      "\t\t##t: tensor([-0.8461,  0.3189,  0.4366])\n",
      "\t\tpersons: tensor([-0.3259,  0.9707,  0.0691])\n",
      "\t\tname: tensor([ 1.3567,  1.1110, -0.7025])\n",
      "\t\tbut: tensor([-0.4271,  0.3427,  0.6771])\n",
      "\t\tim: tensor([-0.3564,  1.2325,  0.5914])\n",
      "\t\ttoo: tensor([ 0.0189, -0.1051,  1.0877])\n",
      "\t\tshy: tensor([-0.7045,  0.5355,  0.7859])\n",
      "\t\tto: tensor([0.2473, 0.0446, 0.4435])\n",
      "\t\tdo: tensor([0.6843, 0.4309, 0.0758])\n",
      "\t\tth: tensor([-0.5547,  0.2692,  0.3823])\n",
      "\t\t##t: tensor([-0.7054,  0.3128,  0.1103])\n",
      "\t\ttoo: tensor([-0.5876, -0.5839, -1.0709])\n",
      "\t\t[SEP]: tensor([ 0.0020, -0.0070,  0.0320])\n",
      "layer: 9 shape: torch.Size([1, 26, 768])\n",
      "\t length of a sentence is 26\n",
      "\t\t[CLS]: tensor([ 0.4035, -0.0879, -0.2087])\n",
      "\t\tim: tensor([0.0601, 1.0644, 0.8051])\n",
      "\t\tcool: tensor([ 0.7345, -0.2376,  0.1883])\n",
      "\t\twith: tensor([ 0.3451, -0.1354,  0.1051])\n",
      "\t\tmy: tensor([ 0.4397,  0.3795, -0.3892])\n",
      "\t\tteacher: tensor([ 0.5368,  0.0158, -0.4731])\n",
      "\t\tso: tensor([-0.2975,  0.4871,  0.5606])\n",
      "\t\ti: tensor([ 0.1582,  0.7778, -0.4703])\n",
      "\t\twas: tensor([-0.1750,  0.6051, -0.8528])\n",
      "\t\tgonna: tensor([ 0.7535,  0.4202, -0.6154])\n",
      "\t\task: tensor([0.2153, 0.5703, 0.3972])\n",
      "\t\tfor: tensor([ 0.1738, -0.0839, -0.5310])\n",
      "\t\tth: tensor([-0.5821, -0.0482,  0.7377])\n",
      "\t\t##t: tensor([-0.9965,  0.1700,  0.6933])\n",
      "\t\tpersons: tensor([-0.3134,  1.0893,  0.1424])\n",
      "\t\tname: tensor([ 1.1853,  1.0259, -0.5168])\n",
      "\t\tbut: tensor([-0.5834,  0.6950,  0.5562])\n",
      "\t\tim: tensor([-0.3645,  1.1211,  0.7560])\n",
      "\t\ttoo: tensor([-0.2387, -0.0796,  0.9283])\n",
      "\t\tshy: tensor([-0.6873,  0.5470,  0.3230])\n",
      "\t\tto: tensor([0.5814, 0.4611, 0.1397])\n",
      "\t\tdo: tensor([0.8527, 0.8183, 0.0584])\n",
      "\t\tth: tensor([-0.5805,  0.0932,  0.5593])\n",
      "\t\t##t: tensor([-0.8066,  0.2650,  0.2234])\n",
      "\t\ttoo: tensor([-0.7220,  0.0657, -0.6961])\n",
      "\t\t[SEP]: tensor([0.0185, 0.0440, 0.0420])\n",
      "layer: 10 shape: torch.Size([1, 26, 768])\n",
      "\t length of a sentence is 26\n",
      "\t\t[CLS]: tensor([ 0.3516, -0.2083,  0.1662])\n",
      "\t\tim: tensor([-0.2957,  0.5001,  1.0851])\n",
      "\t\tcool: tensor([ 0.7167, -0.4576,  0.3234])\n",
      "\t\twith: tensor([ 0.4027, -0.2065,  0.2028])\n",
      "\t\tmy: tensor([ 0.8690, -0.1673,  0.1387])\n",
      "\t\tteacher: tensor([ 0.8622,  0.0972, -0.4511])\n",
      "\t\tso: tensor([-0.5040, -0.0061,  0.3328])\n",
      "\t\ti: tensor([0.3318, 0.0045, 0.0531])\n",
      "\t\twas: tensor([ 0.0317, -0.1701, -0.1831])\n",
      "\t\tgonna: tensor([ 0.8542,  0.3917, -0.0868])\n",
      "\t\task: tensor([0.5305, 0.3312, 0.8525])\n",
      "\t\tfor: tensor([ 0.2668, -0.0972,  0.4261])\n",
      "\t\tth: tensor([-1.0447, -0.2414,  1.1080])\n",
      "\t\t##t: tensor([-1.6423,  0.0731,  0.7448])\n",
      "\t\tpersons: tensor([-0.8167,  0.9104,  0.4067])\n",
      "\t\tname: tensor([ 0.9342,  1.0088, -0.4305])\n",
      "\t\tbut: tensor([-0.4139, -0.0098,  0.2762])\n",
      "\t\tim: tensor([-0.7367,  0.6222,  0.9379])\n",
      "\t\ttoo: tensor([-0.4847, -0.1974,  1.0177])\n",
      "\t\tshy: tensor([-0.6761,  0.5648,  0.7578])\n",
      "\t\tto: tensor([0.4220, 0.1110, 0.4943])\n",
      "\t\tdo: tensor([0.7707, 1.0526, 0.4923])\n",
      "\t\tth: tensor([-0.9318, -0.1843,  1.1717])\n",
      "\t\t##t: tensor([-1.2606,  0.2620,  0.3831])\n",
      "\t\ttoo: tensor([-0.7297, -0.1701, -0.3237])\n",
      "\t\t[SEP]: tensor([ 0.0386,  0.0205, -0.0330])\n",
      "layer: 11 shape: torch.Size([1, 26, 768])\n",
      "\t length of a sentence is 26\n",
      "\t\t[CLS]: tensor([ 0.1801, -0.3019,  0.1785])\n",
      "\t\tim: tensor([-0.4916,  0.5924,  1.1782])\n",
      "\t\tcool: tensor([ 0.4541, -0.5681,  0.3936])\n",
      "\t\twith: tensor([-0.0026, -0.0975,  0.5422])\n",
      "\t\tmy: tensor([ 0.3872, -0.1905,  0.0842])\n",
      "\t\tteacher: tensor([ 0.4381,  0.0092, -0.5003])\n",
      "\t\tso: tensor([-0.5669, -0.0998,  0.3983])\n",
      "\t\ti: tensor([ 0.1716, -0.0268,  0.1577])\n",
      "\t\twas: tensor([-0.5107, -0.2772,  0.1682])\n",
      "\t\tgonna: tensor([ 0.0908,  0.2740, -0.1219])\n",
      "\t\task: tensor([0.2106, 0.8801, 0.8647])\n",
      "\t\tfor: tensor([0.1566, 0.4019, 0.4268])\n",
      "\t\tth: tensor([-0.8462,  0.2755,  1.2082])\n",
      "\t\t##t: tensor([-1.6755,  0.2965,  0.8135])\n",
      "\t\tpersons: tensor([-0.9528,  1.0099, -0.2367])\n",
      "\t\tname: tensor([ 0.6760,  1.0856, -0.4083])\n",
      "\t\tbut: tensor([-0.8833, -0.2249,  0.0307])\n",
      "\t\tim: tensor([-0.8575,  0.8010,  1.1013])\n",
      "\t\ttoo: tensor([-0.8563,  0.0132,  0.8112])\n",
      "\t\tshy: tensor([-0.7424,  0.6546,  0.4512])\n",
      "\t\tto: tensor([-0.0433, -0.0187,  0.0746])\n",
      "\t\tdo: tensor([0.6472, 1.4964, 0.1787])\n",
      "\t\tth: tensor([-0.8205,  0.2713,  1.2139])\n",
      "\t\t##t: tensor([-1.1852,  0.4115,  0.5148])\n",
      "\t\ttoo: tensor([-0.8956, -0.5096, -0.3468])\n",
      "\t\t[SEP]: tensor([ 0.0238,  0.0493, -0.0381])\n",
      "layer: 12 shape: torch.Size([1, 26, 768])\n",
      "\t length of a sentence is 26\n",
      "\t\t[CLS]: tensor([-0.2859,  0.2156,  0.3661])\n",
      "\t\tim: tensor([-0.5151,  0.7284,  1.4654])\n",
      "\t\tcool: tensor([ 0.2690, -0.4727,  0.5947])\n",
      "\t\twith: tensor([ 0.0819, -0.0084,  0.4346])\n",
      "\t\tmy: tensor([0.0731, 0.3009, 0.1285])\n",
      "\t\tteacher: tensor([0.1168, 0.0321, 0.0058])\n",
      "\t\tso: tensor([ 0.0169, -0.0802,  0.4373])\n",
      "\t\ti: tensor([0.1016, 0.0128, 0.1172])\n",
      "\t\twas: tensor([-0.5960,  0.0478, -0.0335])\n",
      "\t\tgonna: tensor([0.0798, 0.4431, 0.0267])\n",
      "\t\task: tensor([0.1379, 0.7309, 0.6095])\n",
      "\t\tfor: tensor([0.2541, 0.3970, 0.4108])\n",
      "\t\tth: tensor([-0.2585,  0.2465,  1.5171])\n",
      "\t\t##t: tensor([-0.7990, -0.0176,  0.8249])\n",
      "\t\tpersons: tensor([-0.5630,  0.3714,  0.0243])\n",
      "\t\tname: tensor([ 0.3575,  0.6121, -0.2158])\n",
      "\t\tbut: tensor([-0.1798, -0.0809,  0.5950])\n",
      "\t\tim: tensor([-0.6990,  0.8047,  1.4387])\n",
      "\t\ttoo: tensor([-0.8761,  0.0474,  0.6335])\n",
      "\t\tshy: tensor([-0.5926,  0.3176,  0.4805])\n",
      "\t\tto: tensor([0.0288, 0.0413, 0.1027])\n",
      "\t\tdo: tensor([0.2665, 0.9414, 0.1970])\n",
      "\t\tth: tensor([-0.1721,  0.2639,  1.5506])\n",
      "\t\t##t: tensor([-0.2908,  0.1180,  0.6058])\n",
      "\t\ttoo: tensor([-0.1379, -0.5360, -0.0885])\n",
      "\t\t[SEP]: tensor([0.6474, 0.4657, 0.0542])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "## use BERT to evaluate our input\n",
    "\n",
    "## [POINT] torch.no_grad will tell pytorch to not make the computing graph on the forward pass.\n",
    "## the forward pass is used during backprop, but since we only need the encoder's output states, we don't need the graph\n",
    "\n",
    "## [POINT] BaseModelOutput (base model's output) returns [0] Last layer's hidden layer [1] hidden layer [2] attentions\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segment_tensor)\n",
    "\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "    layer_id = 0\n",
    "\n",
    "    for layer in hidden_states:\n",
    "        print(f\"layer: {layer_id} shape: {layer.shape}\")\n",
    "\n",
    "        print(f\"\\t length of a sentence is {len(layer[0])}\")\n",
    "\n",
    "        sentence_hidden_layer = layer[0]\n",
    "\n",
    "        for word, output in zip(tokenized_text, sentence_hidden_layer):\n",
    "            print(f\"\\t\\t{word}: {output[:3]}\")\n",
    "\n",
    "        layer_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
