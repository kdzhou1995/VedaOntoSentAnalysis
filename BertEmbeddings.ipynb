{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kdzhou\\.conda\\envs\\PyTorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import logging\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defines tokenizer to tokenize input.\n",
    "## All words unknown to the vocabulary will be split into subwords all the way down to invidual characters\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Sometimes I am so sick of being unproductive that I have dreams where I am being chased by deadlines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sometimes', 'I', 'am', 'so', 'sick', 'of', 'being', 'unproductive', 'that', 'I', 'have', 'dreams', 'where', 'I', 'am', 'being', 'chased', 'by', 'deadlines']\n",
      "['[CLS]', 'sometimes', 'i', 'am', 'so', 'sick', 'of', 'being', 'un', '##pro', '##ductive', 'that', 'i', 'have', 'dreams', 'where', 'i', 'am', 'being', 'chased', 'by', 'deadline', '##s', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "## adding BERT defined separators in front\n",
    "\n",
    "markedUpText = \"[CLS]\" + text + \"[SEP]\"\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(markedUpText)\n",
    "\n",
    "print(text.split())\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['##!',\n",
       " '##\"',\n",
       " '###',\n",
       " '##$',\n",
       " '##%',\n",
       " '##&',\n",
       " \"##'\",\n",
       " '##(',\n",
       " '##)',\n",
       " '##*',\n",
       " '##+',\n",
       " '##,',\n",
       " '##-',\n",
       " '##.',\n",
       " '##/',\n",
       " '##:',\n",
       " '##;',\n",
       " '##<',\n",
       " '##=',\n",
       " '##>',\n",
       " '##?',\n",
       " '##@',\n",
       " '##[',\n",
       " '##\\\\',\n",
       " '##]',\n",
       " '##^',\n",
       " '##_',\n",
       " '##`',\n",
       " '##{',\n",
       " '##|',\n",
       " '##}',\n",
       " '##~',\n",
       " '##¡',\n",
       " '##¢',\n",
       " '##£',\n",
       " '##¤',\n",
       " '##¥',\n",
       " '##¦',\n",
       " '##§',\n",
       " '##¨',\n",
       " '##©',\n",
       " '##ª',\n",
       " '##«',\n",
       " '##¬',\n",
       " '##®',\n",
       " '##±',\n",
       " '##´',\n",
       " '##µ',\n",
       " '##¶',\n",
       " '##·',\n",
       " '##º',\n",
       " '##»',\n",
       " '##¼',\n",
       " '##¾',\n",
       " '##¿',\n",
       " '##æ',\n",
       " '##ð',\n",
       " '##÷',\n",
       " '##þ',\n",
       " '##đ',\n",
       " '##ħ',\n",
       " '##ŋ',\n",
       " '##œ',\n",
       " '##ƒ',\n",
       " '##ɐ',\n",
       " '##ɑ',\n",
       " '##ɒ',\n",
       " '##ɔ',\n",
       " '##ɕ',\n",
       " '##ə',\n",
       " '##ɡ',\n",
       " '##ɣ',\n",
       " '##ɨ',\n",
       " '##ɪ',\n",
       " '##ɫ',\n",
       " '##ɬ',\n",
       " '##ɯ',\n",
       " '##ɲ',\n",
       " '##ɴ',\n",
       " '##ɹ',\n",
       " '##ɾ',\n",
       " '##ʀ',\n",
       " '##ʁ',\n",
       " '##ʂ',\n",
       " '##ʃ',\n",
       " '##ʉ',\n",
       " '##ʊ',\n",
       " '##ʋ',\n",
       " '##ʌ',\n",
       " '##ʎ',\n",
       " '##ʐ',\n",
       " '##ʑ',\n",
       " '##ʒ',\n",
       " '##ʔ',\n",
       " '##ʰ',\n",
       " '##ʲ',\n",
       " '##ʳ',\n",
       " '##ʷ',\n",
       " '##ʸ',\n",
       " '##ʻ',\n",
       " '##ʼ',\n",
       " '##ʾ',\n",
       " '##ʿ',\n",
       " '##ˈ',\n",
       " '##ˡ',\n",
       " '##ˢ',\n",
       " '##ˣ',\n",
       " '##ˤ',\n",
       " '##β',\n",
       " '##γ',\n",
       " '##δ',\n",
       " '##ε',\n",
       " '##ζ',\n",
       " '##θ',\n",
       " '##κ',\n",
       " '##λ',\n",
       " '##μ',\n",
       " '##ξ',\n",
       " '##ο',\n",
       " '##π',\n",
       " '##ρ',\n",
       " '##σ',\n",
       " '##τ',\n",
       " '##υ',\n",
       " '##φ',\n",
       " '##χ',\n",
       " '##ψ',\n",
       " '##ω',\n",
       " '##б',\n",
       " '##г',\n",
       " '##д',\n",
       " '##ж',\n",
       " '##з',\n",
       " '##м',\n",
       " '##п',\n",
       " '##с',\n",
       " '##у',\n",
       " '##ф',\n",
       " '##х',\n",
       " '##ц',\n",
       " '##ч',\n",
       " '##ш',\n",
       " '##щ',\n",
       " '##ъ',\n",
       " '##э',\n",
       " '##ю',\n",
       " '##ђ',\n",
       " '##є',\n",
       " '##і',\n",
       " '##ј',\n",
       " '##љ',\n",
       " '##њ',\n",
       " '##ћ',\n",
       " '##ӏ',\n",
       " '##ա',\n",
       " '##բ',\n",
       " '##գ',\n",
       " '##դ',\n",
       " '##ե',\n",
       " '##թ',\n",
       " '##ի',\n",
       " '##լ',\n",
       " '##կ',\n",
       " '##հ',\n",
       " '##մ',\n",
       " '##յ',\n",
       " '##ն',\n",
       " '##ո',\n",
       " '##պ',\n",
       " '##ս',\n",
       " '##վ',\n",
       " '##տ',\n",
       " '##ր',\n",
       " '##ւ',\n",
       " '##ք',\n",
       " '##־',\n",
       " '##א',\n",
       " '##ב',\n",
       " '##ג',\n",
       " '##ד',\n",
       " '##ו',\n",
       " '##ז',\n",
       " '##ח',\n",
       " '##ט',\n",
       " '##י',\n",
       " '##ך',\n",
       " '##כ',\n",
       " '##ל',\n",
       " '##ם',\n",
       " '##מ',\n",
       " '##ן',\n",
       " '##נ',\n",
       " '##ס',\n",
       " '##ע',\n",
       " '##ף',\n",
       " '##פ',\n",
       " '##ץ',\n",
       " '##צ',\n",
       " '##ק',\n",
       " '##ר',\n",
       " '##ש',\n",
       " '##ת',\n",
       " '##،',\n",
       " '##ء',\n",
       " '##ب',\n",
       " '##ت',\n",
       " '##ث',\n",
       " '##ج',\n",
       " '##ح',\n",
       " '##خ',\n",
       " '##ذ',\n",
       " '##ز',\n",
       " '##س',\n",
       " '##ش',\n",
       " '##ص',\n",
       " '##ض',\n",
       " '##ط',\n",
       " '##ظ',\n",
       " '##ع',\n",
       " '##غ',\n",
       " '##ـ',\n",
       " '##ف',\n",
       " '##ق',\n",
       " '##ك',\n",
       " '##و',\n",
       " '##ى',\n",
       " '##ٹ',\n",
       " '##پ',\n",
       " '##چ',\n",
       " '##ک',\n",
       " '##گ',\n",
       " '##ں',\n",
       " '##ھ',\n",
       " '##ہ',\n",
       " '##ے',\n",
       " '##अ',\n",
       " '##आ',\n",
       " '##उ',\n",
       " '##ए',\n",
       " '##क',\n",
       " '##ख',\n",
       " '##ग',\n",
       " '##च',\n",
       " '##ज',\n",
       " '##ट',\n",
       " '##ड',\n",
       " '##ण',\n",
       " '##त',\n",
       " '##थ',\n",
       " '##द',\n",
       " '##ध',\n",
       " '##न',\n",
       " '##प',\n",
       " '##ब',\n",
       " '##भ',\n",
       " '##म',\n",
       " '##य',\n",
       " '##र',\n",
       " '##ल',\n",
       " '##व',\n",
       " '##श',\n",
       " '##ष',\n",
       " '##स',\n",
       " '##ह',\n",
       " '##ा',\n",
       " '##ि',\n",
       " '##ी',\n",
       " '##ो',\n",
       " '##।',\n",
       " '##॥',\n",
       " '##ং',\n",
       " '##অ',\n",
       " '##আ',\n",
       " '##ই',\n",
       " '##উ',\n",
       " '##এ',\n",
       " '##ও',\n",
       " '##ক',\n",
       " '##খ',\n",
       " '##গ',\n",
       " '##চ',\n",
       " '##ছ',\n",
       " '##জ',\n",
       " '##ট',\n",
       " '##ড',\n",
       " '##ণ',\n",
       " '##ত',\n",
       " '##থ',\n",
       " '##দ',\n",
       " '##ধ',\n",
       " '##ন',\n",
       " '##প',\n",
       " '##ব',\n",
       " '##ভ',\n",
       " '##ম',\n",
       " '##য',\n",
       " '##র',\n",
       " '##ল',\n",
       " '##শ',\n",
       " '##ষ',\n",
       " '##স',\n",
       " '##হ',\n",
       " '##া',\n",
       " '##ি',\n",
       " '##ী',\n",
       " '##ে',\n",
       " '##க',\n",
       " '##ச',\n",
       " '##ட',\n",
       " '##த',\n",
       " '##ந',\n",
       " '##ன',\n",
       " '##ப',\n",
       " '##ம',\n",
       " '##ய',\n",
       " '##ர',\n",
       " '##ல',\n",
       " '##ள',\n",
       " '##வ',\n",
       " '##ா',\n",
       " '##ி',\n",
       " '##ு',\n",
       " '##ே',\n",
       " '##ை',\n",
       " '##ನ',\n",
       " '##ರ',\n",
       " '##ಾ',\n",
       " '##ක',\n",
       " '##ය',\n",
       " '##ර',\n",
       " '##ල',\n",
       " '##ව',\n",
       " '##ා',\n",
       " '##ก',\n",
       " '##ง',\n",
       " '##ต',\n",
       " '##ท',\n",
       " '##น',\n",
       " '##พ',\n",
       " '##ม',\n",
       " '##ย',\n",
       " '##ร',\n",
       " '##ล',\n",
       " '##ว',\n",
       " '##ส',\n",
       " '##อ',\n",
       " '##า',\n",
       " '##เ',\n",
       " '##་',\n",
       " '##།',\n",
       " '##ག',\n",
       " '##ང',\n",
       " '##ད',\n",
       " '##ན',\n",
       " '##པ',\n",
       " '##བ',\n",
       " '##མ',\n",
       " '##འ',\n",
       " '##ར',\n",
       " '##ལ',\n",
       " '##ས',\n",
       " '##မ',\n",
       " '##ა',\n",
       " '##ბ',\n",
       " '##გ',\n",
       " '##დ',\n",
       " '##ე',\n",
       " '##ვ',\n",
       " '##თ',\n",
       " '##ი',\n",
       " '##კ',\n",
       " '##ლ',\n",
       " '##მ',\n",
       " '##ნ',\n",
       " '##ო',\n",
       " '##რ',\n",
       " '##ს',\n",
       " '##ტ',\n",
       " '##უ',\n",
       " '##ᄀ',\n",
       " '##ᄂ',\n",
       " '##ᄃ',\n",
       " '##ᄅ',\n",
       " '##ᄆ',\n",
       " '##ᄇ',\n",
       " '##ᄉ',\n",
       " '##ᄊ',\n",
       " '##ᄋ',\n",
       " '##ᄌ',\n",
       " '##ᄎ',\n",
       " '##ᄏ',\n",
       " '##ᄐ',\n",
       " '##ᄑ',\n",
       " '##ᄒ',\n",
       " '##ᅡ',\n",
       " '##ᅢ',\n",
       " '##ᅥ',\n",
       " '##ᅦ',\n",
       " '##ᅧ',\n",
       " '##ᅩ',\n",
       " '##ᅪ',\n",
       " '##ᅭ',\n",
       " '##ᅮ',\n",
       " '##ᅯ',\n",
       " '##ᅲ',\n",
       " '##ᅳ',\n",
       " '##ᅴ',\n",
       " '##ᅵ',\n",
       " '##ᆨ',\n",
       " '##ᆫ',\n",
       " '##ᆯ',\n",
       " '##ᆷ',\n",
       " '##ᆸ',\n",
       " '##ᆼ',\n",
       " '##ᴬ',\n",
       " '##ᴮ',\n",
       " '##ᴰ',\n",
       " '##ᴵ',\n",
       " '##ᴺ',\n",
       " '##ᵀ',\n",
       " '##ᵃ',\n",
       " '##ᵇ',\n",
       " '##ᵈ',\n",
       " '##ᵉ',\n",
       " '##ᵍ',\n",
       " '##ᵏ',\n",
       " '##ᵐ',\n",
       " '##ᵒ',\n",
       " '##ᵖ',\n",
       " '##ᵗ',\n",
       " '##ᵘ',\n",
       " '##ᵣ',\n",
       " '##ᵤ',\n",
       " '##ᵥ',\n",
       " '##ᶜ',\n",
       " '##ᶠ',\n",
       " '##‐',\n",
       " '##‑',\n",
       " '##‒',\n",
       " '##–',\n",
       " '##—',\n",
       " '##―',\n",
       " '##‖',\n",
       " '##‘',\n",
       " '##’',\n",
       " '##‚',\n",
       " '##“',\n",
       " '##”',\n",
       " '##„',\n",
       " '##†',\n",
       " '##‡',\n",
       " '##•',\n",
       " '##…',\n",
       " '##‰',\n",
       " '##′',\n",
       " '##″',\n",
       " '##›',\n",
       " '##‿',\n",
       " '##⁄',\n",
       " '##⁰',\n",
       " '##ⁱ',\n",
       " '##⁴',\n",
       " '##⁵',\n",
       " '##⁶',\n",
       " '##⁷',\n",
       " '##⁸',\n",
       " '##⁹',\n",
       " '##⁻',\n",
       " '##ⁿ',\n",
       " '##₅',\n",
       " '##₆',\n",
       " '##₇',\n",
       " '##₈',\n",
       " '##₉',\n",
       " '##₊',\n",
       " '##₍',\n",
       " '##₎',\n",
       " '##ₐ',\n",
       " '##ₑ',\n",
       " '##ₒ',\n",
       " '##ₓ',\n",
       " '##ₕ',\n",
       " '##ₖ',\n",
       " '##ₗ',\n",
       " '##ₘ',\n",
       " '##ₚ',\n",
       " '##ₛ',\n",
       " '##ₜ',\n",
       " '##₤',\n",
       " '##₩',\n",
       " '##€',\n",
       " '##₱',\n",
       " '##₹',\n",
       " '##ℓ',\n",
       " '##№',\n",
       " '##ℝ',\n",
       " '##™',\n",
       " '##⅓',\n",
       " '##⅔',\n",
       " '##←',\n",
       " '##↑',\n",
       " '##→',\n",
       " '##↓',\n",
       " '##↔',\n",
       " '##↦',\n",
       " '##⇄',\n",
       " '##⇌',\n",
       " '##⇒',\n",
       " '##∂',\n",
       " '##∅',\n",
       " '##∆',\n",
       " '##∇',\n",
       " '##∈',\n",
       " '##∗',\n",
       " '##∘',\n",
       " '##√',\n",
       " '##∞',\n",
       " '##∧',\n",
       " '##∨',\n",
       " '##∩',\n",
       " '##∪',\n",
       " '##≈',\n",
       " '##≡',\n",
       " '##≤',\n",
       " '##≥',\n",
       " '##⊂',\n",
       " '##⊆',\n",
       " '##⊕',\n",
       " '##⊗',\n",
       " '##⋅',\n",
       " '##─',\n",
       " '##│',\n",
       " '##■',\n",
       " '##▪',\n",
       " '##●',\n",
       " '##★',\n",
       " '##☆',\n",
       " '##☉',\n",
       " '##♠',\n",
       " '##♣',\n",
       " '##♥',\n",
       " '##♦',\n",
       " '##♯',\n",
       " '##⟨',\n",
       " '##⟩',\n",
       " '##ⱼ',\n",
       " '##⺩',\n",
       " '##⺼',\n",
       " '##⽥',\n",
       " '##、',\n",
       " '##。',\n",
       " '##〈',\n",
       " '##〉',\n",
       " '##《',\n",
       " '##》',\n",
       " '##「',\n",
       " '##」',\n",
       " '##『',\n",
       " '##』',\n",
       " '##〜',\n",
       " '##あ',\n",
       " '##い',\n",
       " '##う',\n",
       " '##え',\n",
       " '##お',\n",
       " '##か',\n",
       " '##き',\n",
       " '##く',\n",
       " '##け',\n",
       " '##こ',\n",
       " '##さ',\n",
       " '##し',\n",
       " '##す',\n",
       " '##せ',\n",
       " '##そ',\n",
       " '##た',\n",
       " '##ち',\n",
       " '##っ',\n",
       " '##つ',\n",
       " '##て',\n",
       " '##と',\n",
       " '##な',\n",
       " '##に',\n",
       " '##ぬ',\n",
       " '##ね',\n",
       " '##の',\n",
       " '##は',\n",
       " '##ひ',\n",
       " '##ふ',\n",
       " '##へ',\n",
       " '##ほ',\n",
       " '##ま',\n",
       " '##み',\n",
       " '##む',\n",
       " '##め',\n",
       " '##も',\n",
       " '##や',\n",
       " '##ゆ',\n",
       " '##よ',\n",
       " '##ら',\n",
       " '##り',\n",
       " '##る',\n",
       " '##れ',\n",
       " '##ろ',\n",
       " '##を',\n",
       " '##ん',\n",
       " '##ァ',\n",
       " '##ア',\n",
       " '##ィ',\n",
       " '##イ',\n",
       " '##ウ',\n",
       " '##ェ',\n",
       " '##エ',\n",
       " '##オ',\n",
       " '##カ',\n",
       " '##キ',\n",
       " '##ク',\n",
       " '##ケ',\n",
       " '##コ',\n",
       " '##サ',\n",
       " '##シ',\n",
       " '##ス',\n",
       " '##セ',\n",
       " '##タ',\n",
       " '##チ',\n",
       " '##ッ',\n",
       " '##ツ',\n",
       " '##テ',\n",
       " '##ト',\n",
       " '##ナ',\n",
       " '##ニ',\n",
       " '##ノ',\n",
       " '##ハ',\n",
       " '##ヒ',\n",
       " '##フ',\n",
       " '##ヘ',\n",
       " '##ホ',\n",
       " '##マ',\n",
       " '##ミ',\n",
       " '##ム',\n",
       " '##メ',\n",
       " '##モ',\n",
       " '##ャ',\n",
       " '##ュ',\n",
       " '##ョ',\n",
       " '##ラ',\n",
       " '##リ',\n",
       " '##ル',\n",
       " '##レ',\n",
       " '##ロ',\n",
       " '##ワ',\n",
       " '##ン',\n",
       " '##・',\n",
       " '##ー',\n",
       " '##一',\n",
       " '##三',\n",
       " '##上',\n",
       " '##下',\n",
       " '##不',\n",
       " '##世',\n",
       " '##中',\n",
       " '##主',\n",
       " '##久',\n",
       " '##之',\n",
       " '##也',\n",
       " '##事',\n",
       " '##二',\n",
       " '##五',\n",
       " '##井',\n",
       " '##京',\n",
       " '##人',\n",
       " '##亻',\n",
       " '##仁',\n",
       " '##介',\n",
       " '##代',\n",
       " '##仮',\n",
       " '##伊',\n",
       " '##会',\n",
       " '##佐',\n",
       " '##侍',\n",
       " '##保',\n",
       " '##信',\n",
       " '##健',\n",
       " '##元',\n",
       " '##光',\n",
       " '##八',\n",
       " '##公',\n",
       " '##内',\n",
       " '##出',\n",
       " '##分',\n",
       " '##前',\n",
       " '##劉',\n",
       " '##力',\n",
       " '##加',\n",
       " '##勝',\n",
       " '##北',\n",
       " '##区',\n",
       " '##十',\n",
       " '##千',\n",
       " '##南',\n",
       " '##博',\n",
       " '##原',\n",
       " '##口',\n",
       " '##古',\n",
       " '##史',\n",
       " '##司',\n",
       " '##合',\n",
       " '##吉',\n",
       " '##同',\n",
       " '##名',\n",
       " '##和',\n",
       " '##囗',\n",
       " '##四',\n",
       " '##国',\n",
       " '##國',\n",
       " '##土',\n",
       " '##地',\n",
       " '##坂',\n",
       " '##城',\n",
       " '##堂',\n",
       " '##場',\n",
       " '##士',\n",
       " '##夏',\n",
       " '##外',\n",
       " '##大',\n",
       " '##天',\n",
       " '##太',\n",
       " '##夫',\n",
       " '##奈',\n",
       " '##女',\n",
       " '##子',\n",
       " '##学',\n",
       " '##宀',\n",
       " '##宇',\n",
       " '##安',\n",
       " '##宗',\n",
       " '##定',\n",
       " '##宣',\n",
       " '##宮',\n",
       " '##家',\n",
       " '##宿',\n",
       " '##寺',\n",
       " '##將',\n",
       " '##小',\n",
       " '##尚',\n",
       " '##山',\n",
       " '##岡',\n",
       " '##島',\n",
       " '##崎',\n",
       " '##川',\n",
       " '##州',\n",
       " '##巿',\n",
       " '##帝',\n",
       " '##平',\n",
       " '##年',\n",
       " '##幸',\n",
       " '##广',\n",
       " '##弘',\n",
       " '##張',\n",
       " '##彳',\n",
       " '##後',\n",
       " '##御',\n",
       " '##德',\n",
       " '##心',\n",
       " '##忄',\n",
       " '##志',\n",
       " '##忠',\n",
       " '##愛',\n",
       " '##成',\n",
       " '##我',\n",
       " '##戦',\n",
       " '##戸',\n",
       " '##手',\n",
       " '##扌',\n",
       " '##政',\n",
       " '##文',\n",
       " '##新',\n",
       " '##方',\n",
       " '##日',\n",
       " '##明',\n",
       " '##星',\n",
       " '##春',\n",
       " '##昭',\n",
       " '##智',\n",
       " '##曲',\n",
       " '##書',\n",
       " '##月',\n",
       " '##有',\n",
       " '##朝',\n",
       " '##木',\n",
       " '##本',\n",
       " '##李',\n",
       " '##村',\n",
       " '##東',\n",
       " '##松',\n",
       " '##林',\n",
       " '##森',\n",
       " '##楊',\n",
       " '##樹',\n",
       " '##橋',\n",
       " '##歌',\n",
       " '##止',\n",
       " '##正',\n",
       " '##武',\n",
       " '##比',\n",
       " '##氏',\n",
       " '##民',\n",
       " '##水',\n",
       " '##氵',\n",
       " '##氷',\n",
       " '##永',\n",
       " '##江',\n",
       " '##沢',\n",
       " '##河',\n",
       " '##治',\n",
       " '##法',\n",
       " '##海',\n",
       " '##清',\n",
       " '##漢',\n",
       " '##瀬',\n",
       " '##火',\n",
       " '##版',\n",
       " '##犬',\n",
       " '##王',\n",
       " '##生',\n",
       " '##田',\n",
       " '##男',\n",
       " '##疒',\n",
       " '##発',\n",
       " '##白',\n",
       " '##的',\n",
       " '##皇',\n",
       " '##目',\n",
       " '##相',\n",
       " '##省',\n",
       " '##真',\n",
       " '##石',\n",
       " '##示',\n",
       " '##社',\n",
       " '##神',\n",
       " '##福',\n",
       " '##禾',\n",
       " '##秀',\n",
       " '##秋',\n",
       " '##空',\n",
       " '##立',\n",
       " '##章',\n",
       " '##竹',\n",
       " '##糹',\n",
       " '##美',\n",
       " '##義',\n",
       " '##耳',\n",
       " '##良',\n",
       " '##艹',\n",
       " '##花',\n",
       " '##英',\n",
       " '##華',\n",
       " '##葉',\n",
       " '##藤',\n",
       " '##行',\n",
       " '##街',\n",
       " '##西',\n",
       " '##見',\n",
       " '##訁',\n",
       " '##語',\n",
       " '##谷',\n",
       " '##貝',\n",
       " '##貴',\n",
       " '##車',\n",
       " '##軍',\n",
       " '##辶',\n",
       " '##道',\n",
       " '##郎',\n",
       " '##郡',\n",
       " '##部',\n",
       " '##都',\n",
       " '##里',\n",
       " '##野',\n",
       " '##金',\n",
       " '##鈴',\n",
       " '##镇',\n",
       " '##長',\n",
       " '##門',\n",
       " '##間',\n",
       " '##阝',\n",
       " '##阿',\n",
       " '##陳',\n",
       " '##陽',\n",
       " '##雄',\n",
       " '##青',\n",
       " '##面',\n",
       " '##風',\n",
       " '##食',\n",
       " '##香',\n",
       " '##馬',\n",
       " '##高',\n",
       " '##龍',\n",
       " '##龸',\n",
       " '##ﬁ',\n",
       " '##ﬂ',\n",
       " '##！',\n",
       " '##（',\n",
       " '##）',\n",
       " '##，',\n",
       " '##－',\n",
       " '##．',\n",
       " '##／',\n",
       " '##：',\n",
       " '##？',\n",
       " '##～']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## exploring the vocabulary of the BERT tokenizer\n",
    "\n",
    "## [0] - [PAD]\n",
    "## [100] - [UNK]\n",
    "## [101] - [CLS]\n",
    "## [102] - [SEP]\n",
    "## [104] - [MASK]\n",
    "## [999:1996] are starting characters ! ... ~\n",
    "## [1996:29612] are words\n",
    "## [29612:] are subwords\n",
    "\n",
    "\n",
    "list(tokenizer.vocab.keys())[29612:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 101\n",
      "sometimes 2823\n",
      "i 1045\n",
      "am 2572\n",
      "so 2061\n",
      "sick 5305\n",
      "of 1997\n",
      "being 2108\n",
      "un 4895\n",
      "##pro 21572\n",
      "##ductive 26638\n",
      "that 2008\n",
      "i 1045\n",
      "have 2031\n",
      "dreams 5544\n",
      "where 2073\n",
      "i 1045\n",
      "am 2572\n",
      "being 2108\n",
      "chased 13303\n",
      "by 2011\n",
      "deadline 15117\n",
      "##s 2015\n",
      "[SEP] 102\n"
     ]
    }
   ],
   "source": [
    "## map words to indices\n",
    "\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "for token, id in zip(tokenized_text, token_ids):\n",
    "    print(token, id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 101 1\n",
      "sometimes 2823 1\n",
      "i 1045 1\n",
      "am 2572 1\n",
      "so 2061 1\n",
      "sick 5305 1\n",
      "of 1997 1\n",
      "being 2108 1\n",
      "un 4895 1\n",
      "##pro 21572 1\n",
      "##ductive 26638 1\n",
      "that 2008 1\n",
      "i 1045 1\n",
      "have 2031 1\n",
      "dreams 5544 1\n",
      "where 2073 1\n",
      "i 1045 1\n",
      "am 2572 1\n",
      "being 2108 1\n",
      "chased 13303 1\n",
      "by 2011 1\n",
      "deadline 15117 1\n",
      "##s 2015 1\n",
      "[SEP] 102 1\n"
     ]
    }
   ],
   "source": [
    "## add sentence ids to each token\n",
    "\n",
    "sentence_ids = [1] * len(token_ids)\n",
    "\n",
    "for token, token_id, sentence_id in zip(tokenized_text, token_ids, sentence_ids):\n",
    "    print(token, token_id, sentence_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2823,  1045,  2572,  2061,  5305,  1997,  2108,  4895, 21572,\n",
      "         26638,  2008,  1045,  2031,  5544,  2073,  1045,  2572,  2108, 13303,\n",
      "          2011, 15117,  2015,   102]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "## We need to turn token id and segment id lists into tensors\n",
    "\n",
    "tokens_tensor = torch.tensor([token_ids])\n",
    "\n",
    "segment_tensor = torch.tensor([sentence_ids])\n",
    "\n",
    "print(tokens_tensor)\n",
    "\n",
    "print(segment_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load the BERT pretrained model\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 24])\n",
      "torch.Size([1, 24])\n",
      "layer: 0 shape: torch.Size([1, 24, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.1686, -0.2858, -0.3261])\n",
      "\t\tsometimes: tensor([-0.8025, -0.1504, -0.4737])\n",
      "\t\ti: tensor([-0.3319,  0.4860, -0.1578])\n",
      "\t\tam: tensor([-0.5092,  0.0347,  0.0977])\n",
      "\t\tso: tensor([-0.3429,  0.2499,  0.3117])\n",
      "\t\tsick: tensor([-0.3673,  0.5709,  0.4238])\n",
      "\t\tof: tensor([-0.3686,  0.1742,  0.0343])\n",
      "\t\tbeing: tensor([-0.0210,  0.3527, -0.6497])\n",
      "\t\tun: tensor([-0.6634, -0.5497,  0.1337])\n",
      "\t\t##pro: tensor([ 0.3372,  0.9769, -0.8495])\n",
      "\t\t##ductive: tensor([ 0.8530, -0.5424, -0.2855])\n",
      "\t\tthat: tensor([-0.9356,  0.6592, -0.4697])\n",
      "\t\ti: tensor([-0.3938,  0.6048, -0.0081])\n",
      "\t\thave: tensor([-0.5228,  0.5126,  0.4224])\n",
      "\t\tdreams: tensor([-0.8008,  0.4791, -0.7845])\n",
      "\t\twhere: tensor([-1.0717,  0.7141, -1.0482])\n",
      "\t\ti: tensor([-0.4299,  0.7476, -0.1223])\n",
      "\t\tam: tensor([-0.5885,  0.3569,  0.3604])\n",
      "\t\tbeing: tensor([-0.1835,  0.6141, -0.5630])\n",
      "\t\tchased: tensor([-0.0855, -0.2292,  0.2734])\n",
      "\t\tby: tensor([0.2931, 0.3750, 0.4052])\n",
      "\t\tdeadline: tensor([-0.1517, -1.1950, -0.3204])\n",
      "\t\t##s: tensor([-0.4762, -0.0660,  0.4777])\n",
      "\t\t[SEP]: tensor([-0.2840,  0.2271,  0.0111])\n",
      "layer: 1 shape: torch.Size([1, 24, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.0557,  0.0051, -0.0801])\n",
      "\t\tsometimes: tensor([-0.9919,  0.0589, -0.4445])\n",
      "\t\ti: tensor([ 0.0843,  0.6711, -0.4110])\n",
      "\t\tam: tensor([ 0.0044, -0.2136,  0.0523])\n",
      "\t\tso: tensor([-0.1585,  0.0783,  0.5165])\n",
      "\t\tsick: tensor([-0.0840,  0.7970,  0.4999])\n",
      "\t\tof: tensor([-0.3993,  0.1225,  0.0610])\n",
      "\t\tbeing: tensor([-0.1161,  0.5155, -1.1653])\n",
      "\t\tun: tensor([-0.8307, -1.1828,  0.0793])\n",
      "\t\t##pro: tensor([ 0.6507,  0.1526, -0.7782])\n",
      "\t\t##ductive: tensor([ 1.0558, -0.4811, -0.9647])\n",
      "\t\tthat: tensor([-1.2546,  1.0354, -0.9712])\n",
      "\t\ti: tensor([0.0480, 0.6568, 0.0149])\n",
      "\t\thave: tensor([-0.2254,  0.8139,  0.8334])\n",
      "\t\tdreams: tensor([-0.7776,  0.4952, -1.4022])\n",
      "\t\twhere: tensor([-1.7670,  1.0344, -1.6430])\n",
      "\t\ti: tensor([-0.0559,  1.1939, -0.2980])\n",
      "\t\tam: tensor([-0.2534,  0.0547,  0.4553])\n",
      "\t\tbeing: tensor([-0.3100,  0.5174, -0.6326])\n",
      "\t\tchased: tensor([ 0.6032, -0.7661,  0.6167])\n",
      "\t\tby: tensor([ 0.6559, -0.2007,  0.6290])\n",
      "\t\tdeadline: tensor([ 0.2932, -1.0677, -0.3050])\n",
      "\t\t##s: tensor([-0.2576, -0.2904,  0.7642])\n",
      "\t\t[SEP]: tensor([0.0791, 0.4475, 0.0070])\n",
      "layer: 2 shape: torch.Size([1, 24, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([-0.0498, -0.1668, -0.2013])\n",
      "\t\tsometimes: tensor([-1.3561,  0.4988, -0.3295])\n",
      "\t\ti: tensor([-0.0437,  0.6007, -0.3393])\n",
      "\t\tam: tensor([ 0.0398, -0.2459,  0.1971])\n",
      "\t\tso: tensor([ 0.4543, -0.3656,  0.8669])\n",
      "\t\tsick: tensor([-0.0884,  0.9500,  0.3963])\n",
      "\t\tof: tensor([-0.3864,  0.0987,  0.1150])\n",
      "\t\tbeing: tensor([ 0.2358,  0.1060, -0.9742])\n",
      "\t\tun: tensor([-0.0578, -0.5485,  0.5409])\n",
      "\t\t##pro: tensor([ 0.7150,  0.3356, -0.8300])\n",
      "\t\t##ductive: tensor([ 0.7876,  0.0307, -1.0253])\n",
      "\t\tthat: tensor([-0.9237,  0.5173, -0.6325])\n",
      "\t\ti: tensor([-0.1083,  0.8078,  0.5674])\n",
      "\t\thave: tensor([0.2097, 0.9033, 1.2805])\n",
      "\t\tdreams: tensor([-0.5174,  0.9624, -1.6169])\n",
      "\t\twhere: tensor([-1.7884,  1.3063, -1.3750])\n",
      "\t\ti: tensor([-0.1591,  1.1073,  0.0511])\n",
      "\t\tam: tensor([-0.2907,  0.1276,  0.6767])\n",
      "\t\tbeing: tensor([ 0.1906,  0.4129, -0.2277])\n",
      "\t\tchased: tensor([ 0.9635, -0.7139,  1.5472])\n",
      "\t\tby: tensor([ 0.4508, -0.2813,  1.1355])\n",
      "\t\tdeadline: tensor([ 0.4668, -1.2314, -0.4018])\n",
      "\t\t##s: tensor([-0.1500, -0.5188,  1.0378])\n",
      "\t\t[SEP]: tensor([-0.0247,  0.1889,  0.1587])\n",
      "layer: 3 shape: torch.Size([1, 24, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.0772, -0.3436, -0.1532])\n",
      "\t\tsometimes: tensor([-1.3031,  0.5432,  0.0425])\n",
      "\t\ti: tensor([ 0.4193,  0.2846, -0.1992])\n",
      "\t\tam: tensor([ 0.2016, -0.1263, -0.0462])\n",
      "\t\tso: tensor([ 0.5526, -0.2547,  1.3434])\n",
      "\t\tsick: tensor([0.2618, 0.7151, 0.3673])\n",
      "\t\tof: tensor([-0.2551, -0.2109,  0.7222])\n",
      "\t\tbeing: tensor([ 0.7241,  0.0751, -1.0202])\n",
      "\t\tun: tensor([ 0.1917, -0.7135,  0.9949])\n",
      "\t\t##pro: tensor([ 0.7554, -0.0096, -1.1123])\n",
      "\t\t##ductive: tensor([ 0.4586,  0.0806, -1.2606])\n",
      "\t\tthat: tensor([-0.4013,  0.6850, -0.5405])\n",
      "\t\ti: tensor([0.4436, 0.5278, 0.7574])\n",
      "\t\thave: tensor([-0.0229,  1.1486,  1.1287])\n",
      "\t\tdreams: tensor([-0.3396,  1.0654, -1.0990])\n",
      "\t\twhere: tensor([-1.6283,  0.8183, -1.4275])\n",
      "\t\ti: tensor([0.4252, 0.7480, 0.2165])\n",
      "\t\tam: tensor([-0.2807,  0.1013,  0.7117])\n",
      "\t\tbeing: tensor([0.6426, 0.2575, 0.0007])\n",
      "\t\tchased: tensor([ 0.9215, -0.8575,  1.5043])\n",
      "\t\tby: tensor([ 0.4226, -0.0105,  1.0488])\n",
      "\t\tdeadline: tensor([ 0.6902, -0.7589, -0.0371])\n",
      "\t\t##s: tensor([-0.0807, -0.4817,  0.9747])\n",
      "\t\t[SEP]: tensor([-0.0206, -0.0758,  0.1099])\n",
      "layer: 4 shape: torch.Size([1, 24, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.3199, -0.6800, -0.3782])\n",
      "\t\tsometimes: tensor([-1.1417,  0.0196,  0.2176])\n",
      "\t\ti: tensor([ 0.4711,  0.1911, -0.3396])\n",
      "\t\tam: tensor([ 0.1154, -0.8227,  0.1180])\n",
      "\t\tso: tensor([ 0.1042, -0.0793,  1.3779])\n",
      "\t\tsick: tensor([0.3801, 0.2071, 0.1057])\n",
      "\t\tof: tensor([-0.0467, -0.2311,  0.9109])\n",
      "\t\tbeing: tensor([ 0.4062,  0.0553, -1.2166])\n",
      "\t\tun: tensor([-0.0925, -0.5841,  0.7859])\n",
      "\t\t##pro: tensor([ 0.8659, -0.6431, -0.7675])\n",
      "\t\t##ductive: tensor([ 0.4536,  0.0384, -1.2294])\n",
      "\t\tthat: tensor([-0.4058,  0.3825, -0.1945])\n",
      "\t\ti: tensor([0.6570, 0.4145, 0.5616])\n",
      "\t\thave: tensor([-0.1205,  0.9674,  1.3075])\n",
      "\t\tdreams: tensor([-0.0866,  0.7580, -0.7157])\n",
      "\t\twhere: tensor([-1.8196,  1.3392, -1.0181])\n",
      "\t\ti: tensor([0.3924, 0.6971, 0.0080])\n",
      "\t\tam: tensor([-0.2125, -0.0727,  0.7014])\n",
      "\t\tbeing: tensor([ 0.5213, -0.0902, -0.2188])\n",
      "\t\tchased: tensor([ 1.4115, -0.8558,  1.0145])\n",
      "\t\tby: tensor([ 0.4937, -0.4591,  0.6745])\n",
      "\t\tdeadline: tensor([ 0.9523, -1.1034,  0.1122])\n",
      "\t\t##s: tensor([ 0.0600, -0.7960,  0.4632])\n",
      "\t\t[SEP]: tensor([-0.0146, -0.0479,  0.0197])\n",
      "layer: 5 shape: torch.Size([1, 24, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.3291, -0.6729, -0.2263])\n",
      "\t\tsometimes: tensor([-1.0814,  0.2441, -0.2049])\n",
      "\t\ti: tensor([ 0.3878,  0.3872, -0.4509])\n",
      "\t\tam: tensor([ 0.4809, -0.4487, -0.5310])\n",
      "\t\tso: tensor([ 0.2402, -0.1798,  0.9977])\n",
      "\t\tsick: tensor([-0.1125,  0.0496,  0.0940])\n",
      "\t\tof: tensor([-0.1102, -0.0806,  0.3141])\n",
      "\t\tbeing: tensor([ 0.8083, -0.1273, -1.2376])\n",
      "\t\tun: tensor([-0.3064, -0.6764,  0.9329])\n",
      "\t\t##pro: tensor([ 0.7072, -0.5570, -0.8887])\n",
      "\t\t##ductive: tensor([ 0.3787,  0.1576, -1.1277])\n",
      "\t\tthat: tensor([-0.3327,  0.9774, -0.1002])\n",
      "\t\ti: tensor([0.6037, 0.5786, 0.2223])\n",
      "\t\thave: tensor([0.3856, 1.3422, 0.9631])\n",
      "\t\tdreams: tensor([ 0.0882,  0.9170, -0.8567])\n",
      "\t\twhere: tensor([-1.6601,  1.3600, -0.8970])\n",
      "\t\ti: tensor([0.3887, 0.9566, 0.0234])\n",
      "\t\tam: tensor([-0.1317,  0.4201, -0.0308])\n",
      "\t\tbeing: tensor([ 0.3364, -0.3925, -0.5071])\n",
      "\t\tchased: tensor([ 1.9127, -0.9252,  0.6929])\n",
      "\t\tby: tensor([ 0.4597, -0.3535,  0.7179])\n",
      "\t\tdeadline: tensor([ 1.0572, -1.0125, -0.1135])\n",
      "\t\t##s: tensor([ 0.1295, -0.5718,  0.3454])\n",
      "\t\t[SEP]: tensor([-0.0180, -0.0359,  0.0257])\n",
      "layer: 6 shape: torch.Size([1, 24, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.4059, -0.7234, -0.3523])\n",
      "\t\tsometimes: tensor([-1.0519,  0.4012, -0.2107])\n",
      "\t\ti: tensor([-0.0387,  0.5462, -0.5431])\n",
      "\t\tam: tensor([ 0.3891, -0.3604, -0.8538])\n",
      "\t\tso: tensor([-0.1770, -0.0259,  0.2411])\n",
      "\t\tsick: tensor([-0.0178, -0.2721,  0.1629])\n",
      "\t\tof: tensor([-0.4076, -0.3170,  0.2254])\n",
      "\t\tbeing: tensor([-0.0644, -0.4458, -1.3567])\n",
      "\t\tun: tensor([-0.4696, -0.3083,  0.8068])\n",
      "\t\t##pro: tensor([ 0.4129, -0.4263, -0.4982])\n",
      "\t\t##ductive: tensor([ 0.3563,  0.4202, -0.9429])\n",
      "\t\tthat: tensor([-0.0211,  0.6241, -0.3070])\n",
      "\t\ti: tensor([0.3954, 0.7598, 0.2513])\n",
      "\t\thave: tensor([0.4134, 1.1656, 1.1199])\n",
      "\t\tdreams: tensor([-0.1150,  0.9303, -0.2690])\n",
      "\t\twhere: tensor([-1.8385,  1.3739, -0.5571])\n",
      "\t\ti: tensor([ 0.1822,  0.7156, -0.1821])\n",
      "\t\tam: tensor([0.0515, 0.1793, 0.0090])\n",
      "\t\tbeing: tensor([ 0.2717, -0.8414, -0.6280])\n",
      "\t\tchased: tensor([ 1.7749, -0.8079,  0.8304])\n",
      "\t\tby: tensor([ 0.0212, -0.2259,  0.3370])\n",
      "\t\tdeadline: tensor([ 0.9672, -0.7723, -0.2552])\n",
      "\t\t##s: tensor([-0.0361, -0.5169,  0.0477])\n",
      "\t\t[SEP]: tensor([ 0.0199, -0.0408, -0.0079])\n",
      "layer: 7 shape: torch.Size([1, 24, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.5206, -0.6003, -0.3172])\n",
      "\t\tsometimes: tensor([-1.2383,  0.4782, -0.6084])\n",
      "\t\ti: tensor([-0.2837,  0.1348, -0.6848])\n",
      "\t\tam: tensor([ 0.0639, -0.3345, -0.8915])\n",
      "\t\tso: tensor([-0.2711,  0.0685,  0.2256])\n",
      "\t\tsick: tensor([ 0.2151, -0.6595, -0.1160])\n",
      "\t\tof: tensor([-0.4949, -0.4660,  0.4152])\n",
      "\t\tbeing: tensor([-0.1864, -0.6258, -1.4702])\n",
      "\t\tun: tensor([-0.2184, -0.6340,  0.5013])\n",
      "\t\t##pro: tensor([ 0.0251, -0.6659, -0.0765])\n",
      "\t\t##ductive: tensor([ 0.2160,  0.4925, -1.1524])\n",
      "\t\tthat: tensor([-0.4830,  0.2503, -0.1106])\n",
      "\t\ti: tensor([ 0.1109,  0.2334, -0.2796])\n",
      "\t\thave: tensor([0.0453, 1.2321, 0.8906])\n",
      "\t\tdreams: tensor([-0.5106,  0.8814, -0.4108])\n",
      "\t\twhere: tensor([-1.7694,  0.7759, -0.3237])\n",
      "\t\ti: tensor([-0.0176, -0.0865, -0.5106])\n",
      "\t\tam: tensor([ 0.0353, -0.0907, -0.2026])\n",
      "\t\tbeing: tensor([ 0.1041, -1.0825, -0.3757])\n",
      "\t\tchased: tensor([ 1.4903, -0.7588,  1.0910])\n",
      "\t\tby: tensor([-0.3194, -0.4987,  0.6541])\n",
      "\t\tdeadline: tensor([ 1.0420, -0.6792, -0.3657])\n",
      "\t\t##s: tensor([-0.0151, -1.3501, -0.6887])\n",
      "\t\t[SEP]: tensor([ 0.0183, -0.0152, -0.0171])\n",
      "layer: 8 shape: torch.Size([1, 24, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.7072, -0.2997, -0.7249])\n",
      "\t\tsometimes: tensor([-1.1655,  0.4780, -0.4277])\n",
      "\t\ti: tensor([-0.3051,  0.1988, -0.5944])\n",
      "\t\tam: tensor([-0.0422, -0.0568, -0.2021])\n",
      "\t\tso: tensor([-0.7362, -0.2582,  0.6390])\n",
      "\t\tsick: tensor([ 0.2469, -0.9130,  0.1117])\n",
      "\t\tof: tensor([-0.8709, -0.8603,  0.2992])\n",
      "\t\tbeing: tensor([-0.4722, -0.3381, -0.8033])\n",
      "\t\tun: tensor([-0.2868, -0.6004,  0.0317])\n",
      "\t\t##pro: tensor([-0.0540, -0.5607, -0.2984])\n",
      "\t\t##ductive: tensor([ 0.3652,  0.1647, -0.9500])\n",
      "\t\tthat: tensor([-0.9345,  0.1440, -0.1189])\n",
      "\t\ti: tensor([-0.2649,  0.5239, -0.3559])\n",
      "\t\thave: tensor([-0.6341,  1.4820,  0.3829])\n",
      "\t\tdreams: tensor([-0.3179,  1.0363, -0.5991])\n",
      "\t\twhere: tensor([-1.7810,  0.7260, -0.2182])\n",
      "\t\ti: tensor([-0.0836,  0.2219, -0.4394])\n",
      "\t\tam: tensor([-0.0221,  0.0752, -0.3592])\n",
      "\t\tbeing: tensor([-0.2781, -0.9358, -0.5207])\n",
      "\t\tchased: tensor([ 0.9845, -0.8256,  0.7739])\n",
      "\t\tby: tensor([-0.4405, -0.7500,  0.3395])\n",
      "\t\tdeadline: tensor([ 0.9163, -0.6436, -0.5867])\n",
      "\t\t##s: tensor([-0.4896, -0.9618, -1.2578])\n",
      "\t\t[SEP]: tensor([0.0280, 0.0010, 0.0218])\n",
      "layer: 9 shape: torch.Size([1, 24, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.8539, -0.0228, -0.8334])\n",
      "\t\tsometimes: tensor([-0.8576,  0.5549, -0.0697])\n",
      "\t\ti: tensor([ 0.0879,  0.2420, -0.5097])\n",
      "\t\tam: tensor([ 0.0586, -0.0424, -0.3307])\n",
      "\t\tso: tensor([-0.6444, -0.1522,  0.4281])\n",
      "\t\tsick: tensor([ 0.1739, -0.7149,  0.0467])\n",
      "\t\tof: tensor([-0.5825, -0.0537,  0.3767])\n",
      "\t\tbeing: tensor([-0.1067, -0.0456, -0.6350])\n",
      "\t\tun: tensor([ 0.0645, -0.7416,  0.0063])\n",
      "\t\t##pro: tensor([ 0.2038, -0.6503, -0.2834])\n",
      "\t\t##ductive: tensor([ 0.5901,  0.0134, -0.7292])\n",
      "\t\tthat: tensor([-0.3519,  0.8179, -0.3771])\n",
      "\t\ti: tensor([-0.0785,  0.7424, -0.4493])\n",
      "\t\thave: tensor([-0.5898,  1.5247,  0.1736])\n",
      "\t\tdreams: tensor([ 0.1179,  0.6271, -0.4255])\n",
      "\t\twhere: tensor([-1.5444,  0.5581, -0.2603])\n",
      "\t\ti: tensor([-0.0776,  0.5621, -0.3078])\n",
      "\t\tam: tensor([-0.1809,  0.0969, -0.5291])\n",
      "\t\tbeing: tensor([ 0.0164, -0.9168, -0.4722])\n",
      "\t\tchased: tensor([ 1.0139, -0.9866,  0.6695])\n",
      "\t\tby: tensor([-0.1892, -1.2256,  0.1973])\n",
      "\t\tdeadline: tensor([ 0.6941, -0.7494, -0.3806])\n",
      "\t\t##s: tensor([-0.3568, -0.5859, -1.1737])\n",
      "\t\t[SEP]: tensor([ 0.0333,  0.0148, -0.0188])\n",
      "layer: 10 shape: torch.Size([1, 24, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.6294, -0.0844, -0.5866])\n",
      "\t\tsometimes: tensor([-0.6331,  0.3146,  0.4063])\n",
      "\t\ti: tensor([ 0.3600, -0.2488, -0.0127])\n",
      "\t\tam: tensor([ 0.1420, -0.1762,  0.2235])\n",
      "\t\tso: tensor([-0.5279, -0.4004,  0.2787])\n",
      "\t\tsick: tensor([ 0.4729, -0.3046,  0.0020])\n",
      "\t\tof: tensor([-0.5117,  0.5983,  0.4008])\n",
      "\t\tbeing: tensor([ 0.3389,  0.3536, -0.5836])\n",
      "\t\tun: tensor([ 0.1953, -0.1749,  0.3379])\n",
      "\t\t##pro: tensor([ 0.4580, -0.0122, -0.0617])\n",
      "\t\t##ductive: tensor([ 0.8502,  0.0882, -0.5646])\n",
      "\t\tthat: tensor([-0.7625,  0.8798, -0.2400])\n",
      "\t\ti: tensor([ 0.1905,  0.2969, -0.0862])\n",
      "\t\thave: tensor([-0.7123,  1.4250,  0.3925])\n",
      "\t\tdreams: tensor([0.0686, 0.5810, 0.0766])\n",
      "\t\twhere: tensor([-1.2429,  0.7309,  0.0752])\n",
      "\t\ti: tensor([ 0.1090,  0.0828, -0.1494])\n",
      "\t\tam: tensor([-0.0729,  0.1310, -0.5524])\n",
      "\t\tbeing: tensor([ 0.0904, -0.8457, -0.2667])\n",
      "\t\tchased: tensor([ 1.0708, -1.3162,  0.6533])\n",
      "\t\tby: tensor([-0.0256, -0.9959,  0.6791])\n",
      "\t\tdeadline: tensor([ 0.6342, -0.6702, -0.0912])\n",
      "\t\t##s: tensor([-0.3585, -0.4044, -0.8233])\n",
      "\t\t[SEP]: tensor([ 0.0274,  0.0065, -0.0424])\n",
      "layer: 11 shape: torch.Size([1, 24, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.5942,  0.0272, -0.2067])\n",
      "\t\tsometimes: tensor([-0.3690,  0.3757,  0.6019])\n",
      "\t\ti: tensor([0.6852, 0.2452, 0.0536])\n",
      "\t\tam: tensor([0.2753, 0.3057, 0.3364])\n",
      "\t\tso: tensor([-0.7819, -0.5349, -0.2100])\n",
      "\t\tsick: tensor([ 0.2174, -0.2699, -0.0804])\n",
      "\t\tof: tensor([-0.6493,  0.6049,  0.5739])\n",
      "\t\tbeing: tensor([ 0.4351,  0.2370, -0.3198])\n",
      "\t\tun: tensor([ 0.2548, -0.1474,  0.1600])\n",
      "\t\t##pro: tensor([ 0.2712,  0.0057, -0.1526])\n",
      "\t\t##ductive: tensor([ 1.0486,  0.2037, -0.3513])\n",
      "\t\tthat: tensor([-0.9491,  1.2623,  0.0538])\n",
      "\t\ti: tensor([0.4777, 0.4823, 0.0301])\n",
      "\t\thave: tensor([-0.1902,  1.6791,  0.8089])\n",
      "\t\tdreams: tensor([0.3174, 0.7717, 0.1009])\n",
      "\t\twhere: tensor([-0.9997,  0.7559,  0.5076])\n",
      "\t\ti: tensor([ 0.4072,  0.4555, -0.0606])\n",
      "\t\tam: tensor([ 0.0593,  0.7535, -0.2035])\n",
      "\t\tbeing: tensor([ 0.2578, -0.8466,  0.0566])\n",
      "\t\tchased: tensor([ 1.0964, -1.0142,  0.8380])\n",
      "\t\tby: tensor([ 0.1972, -0.9566,  0.6288])\n",
      "\t\tdeadline: tensor([ 0.7022, -0.4724,  0.0124])\n",
      "\t\t##s: tensor([-0.2539, -0.4869, -0.2939])\n",
      "\t\t[SEP]: tensor([ 0.0362,  0.0367, -0.0293])\n",
      "layer: 12 shape: torch.Size([1, 24, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.5613,  0.0896, -0.1570])\n",
      "\t\tsometimes: tensor([-0.0022,  0.2364,  0.5325])\n",
      "\t\ti: tensor([0.4311, 0.1228, 0.0790])\n",
      "\t\tam: tensor([-0.0116,  0.2984,  0.6138])\n",
      "\t\tso: tensor([-0.6140, -0.3001,  0.0847])\n",
      "\t\tsick: tensor([ 0.4870, -0.3558,  0.1976])\n",
      "\t\tof: tensor([-0.5893,  0.5575,  0.0885])\n",
      "\t\tbeing: tensor([ 0.1253,  0.0403, -0.0894])\n",
      "\t\tun: tensor([ 0.0331, -0.3374,  0.2331])\n",
      "\t\t##pro: tensor([ 0.1838, -0.0970, -0.2647])\n",
      "\t\t##ductive: tensor([ 0.6815,  0.0218, -0.2953])\n",
      "\t\tthat: tensor([-0.3935,  0.8944,  0.1987])\n",
      "\t\ti: tensor([0.3077, 0.2916, 0.2182])\n",
      "\t\thave: tensor([0.1284, 0.8554, 0.9328])\n",
      "\t\tdreams: tensor([0.4492, 0.3384, 0.6839])\n",
      "\t\twhere: tensor([-0.3373,  0.1329,  0.5526])\n",
      "\t\ti: tensor([0.2139, 0.3042, 0.2200])\n",
      "\t\tam: tensor([ 0.1149,  0.6153, -0.0596])\n",
      "\t\tbeing: tensor([ 0.2230, -0.2829, -0.0083])\n",
      "\t\tchased: tensor([ 0.8882, -0.4940,  0.5868])\n",
      "\t\tby: tensor([ 0.1810, -0.1551,  0.3991])\n",
      "\t\tdeadline: tensor([ 0.4241, -0.3513, -0.1401])\n",
      "\t\t##s: tensor([-0.0337, -0.6354, -0.3106])\n",
      "\t\t[SEP]: tensor([ 0.7718,  0.1310, -0.2125])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segment_tensor)\n",
    "\n",
    "    print(tokens_tensor.shape)\n",
    "    print(segment_tensor.shape)\n",
    "\n",
    "    ## output = last layer hidden state, pooler output, hidden state output by layer\n",
    "    hidden_states = outputs[2] # last layer\n",
    "\n",
    "    layer_id = 0\n",
    "\n",
    "    for layer in hidden_states:\n",
    "\n",
    "        print(f\"layer: {layer_id} shape: {layer.shape}\")\n",
    "\n",
    "        print(f\"\\t length of a sentence is {len(layer)}\")\n",
    "\n",
    "        sentence_hidden_layer = layer[0]\n",
    "\n",
    "        for word, output in zip(tokenized_text, sentence_hidden_layer):\n",
    "            print(f\"\\t\\t{word}: {output[:3]}\")\n",
    "\n",
    "        layer_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t[CLS]: tensor([ 0.5613,  0.0896, -0.1570])\n",
      "\t\tsometimes: tensor([-0.0022,  0.2364,  0.5325])\n",
      "\t\ti: tensor([0.4311, 0.1228, 0.0790])\n",
      "\t\tam: tensor([-0.0116,  0.2984,  0.6138])\n",
      "\t\tso: tensor([-0.6140, -0.3001,  0.0847])\n",
      "\t\tsick: tensor([ 0.4870, -0.3558,  0.1976])\n",
      "\t\tof: tensor([-0.5893,  0.5575,  0.0885])\n",
      "\t\tbeing: tensor([ 0.1253,  0.0403, -0.0894])\n",
      "\t\tun: tensor([ 0.0331, -0.3374,  0.2331])\n",
      "\t\t##pro: tensor([ 0.1838, -0.0970, -0.2647])\n",
      "\t\t##ductive: tensor([ 0.6815,  0.0218, -0.2953])\n",
      "\t\tthat: tensor([-0.3935,  0.8944,  0.1987])\n",
      "\t\ti: tensor([0.3077, 0.2916, 0.2182])\n",
      "\t\thave: tensor([0.1284, 0.8554, 0.9328])\n",
      "\t\tdreams: tensor([0.4492, 0.3384, 0.6839])\n",
      "\t\twhere: tensor([-0.3373,  0.1329,  0.5526])\n",
      "\t\ti: tensor([0.2139, 0.3042, 0.2200])\n",
      "\t\tam: tensor([ 0.1149,  0.6153, -0.0596])\n",
      "\t\tbeing: tensor([ 0.2230, -0.2829, -0.0083])\n",
      "\t\tchased: tensor([ 0.8882, -0.4940,  0.5868])\n",
      "\t\tby: tensor([ 0.1810, -0.1551,  0.3991])\n",
      "\t\tdeadline: tensor([ 0.4241, -0.3513, -0.1401])\n",
      "\t\t##s: tensor([-0.0337, -0.6354, -0.3106])\n",
      "\t\t[SEP]: tensor([ 0.7718,  0.1310, -0.2125])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segment_tensor)\n",
    "\n",
    "    hidden_states = outputs[2][-1]\n",
    "\n",
    "    for word, output in zip(tokenized_text, sentence_hidden_layer):\n",
    "        print(f\"\\t\\t{word}: {output[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 13 shape: torch.Size([1, 24, 768])\n"
     ]
    }
   ],
   "source": [
    "## use BERT to evaluate our input\n",
    "\n",
    "## [POINT] torch.no_grad will tell pytorch to not make the computing graph on the forward pass.\n",
    "## the forward pass is used during backprop, but since we only need the encoder's output states, we don't need the graph\n",
    "\n",
    "## [POINT] BaseModelOutput (base model's output) returns [0] Last layer's hidden layer [1] hidden layer [2] attentions\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segment_tensor)\n",
    "\n",
    "    hidden_states = outputs[2][-1]\n",
    "\n",
    "    print(f\"layer: {layer_id} shape: {layer.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
