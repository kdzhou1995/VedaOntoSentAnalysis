{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kdzhou\\.conda\\envs\\PyTorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import logging\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defines tokenizer to tokenize input.\n",
    "## All words unknown to the vocabulary will be split into subwords all the way down to invidual characters\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input sentence\n",
    "text = \"I wish I can be more productive and be less tempted to play video games\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'wish', 'I', 'can', 'be', 'more', 'productive', 'and', 'be', 'less', 'tempted', 'to', 'play', 'video', 'games']\n",
      "['[CLS]', 'i', 'wish', 'i', 'can', 'be', 'more', 'productive', 'and', 'be', 'less', 'tempted', 'to', 'play', 'video', 'games', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "## adding BERT defined separators in front\n",
    "\n",
    "markedUpText = \"[CLS]\" + text + \"[SEP]\"\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(markedUpText)\n",
    "\n",
    "print(text.split())\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['##!',\n",
       " '##\"',\n",
       " '###',\n",
       " '##$',\n",
       " '##%',\n",
       " '##&',\n",
       " \"##'\",\n",
       " '##(',\n",
       " '##)',\n",
       " '##*',\n",
       " '##+',\n",
       " '##,',\n",
       " '##-',\n",
       " '##.',\n",
       " '##/',\n",
       " '##:',\n",
       " '##;',\n",
       " '##<',\n",
       " '##=',\n",
       " '##>',\n",
       " '##?',\n",
       " '##@',\n",
       " '##[',\n",
       " '##\\\\',\n",
       " '##]',\n",
       " '##^',\n",
       " '##_',\n",
       " '##`',\n",
       " '##{',\n",
       " '##|',\n",
       " '##}',\n",
       " '##~',\n",
       " '##¡',\n",
       " '##¢',\n",
       " '##£',\n",
       " '##¤',\n",
       " '##¥',\n",
       " '##¦',\n",
       " '##§',\n",
       " '##¨',\n",
       " '##©',\n",
       " '##ª',\n",
       " '##«',\n",
       " '##¬',\n",
       " '##®',\n",
       " '##±',\n",
       " '##´',\n",
       " '##µ',\n",
       " '##¶',\n",
       " '##·',\n",
       " '##º',\n",
       " '##»',\n",
       " '##¼',\n",
       " '##¾',\n",
       " '##¿',\n",
       " '##æ',\n",
       " '##ð',\n",
       " '##÷',\n",
       " '##þ',\n",
       " '##đ',\n",
       " '##ħ',\n",
       " '##ŋ',\n",
       " '##œ',\n",
       " '##ƒ',\n",
       " '##ɐ',\n",
       " '##ɑ',\n",
       " '##ɒ',\n",
       " '##ɔ',\n",
       " '##ɕ',\n",
       " '##ə',\n",
       " '##ɡ',\n",
       " '##ɣ',\n",
       " '##ɨ',\n",
       " '##ɪ',\n",
       " '##ɫ',\n",
       " '##ɬ',\n",
       " '##ɯ',\n",
       " '##ɲ',\n",
       " '##ɴ',\n",
       " '##ɹ',\n",
       " '##ɾ',\n",
       " '##ʀ',\n",
       " '##ʁ',\n",
       " '##ʂ',\n",
       " '##ʃ',\n",
       " '##ʉ',\n",
       " '##ʊ',\n",
       " '##ʋ',\n",
       " '##ʌ',\n",
       " '##ʎ',\n",
       " '##ʐ',\n",
       " '##ʑ',\n",
       " '##ʒ',\n",
       " '##ʔ',\n",
       " '##ʰ',\n",
       " '##ʲ',\n",
       " '##ʳ',\n",
       " '##ʷ',\n",
       " '##ʸ',\n",
       " '##ʻ',\n",
       " '##ʼ',\n",
       " '##ʾ',\n",
       " '##ʿ',\n",
       " '##ˈ',\n",
       " '##ˡ',\n",
       " '##ˢ',\n",
       " '##ˣ',\n",
       " '##ˤ',\n",
       " '##β',\n",
       " '##γ',\n",
       " '##δ',\n",
       " '##ε',\n",
       " '##ζ',\n",
       " '##θ',\n",
       " '##κ',\n",
       " '##λ',\n",
       " '##μ',\n",
       " '##ξ',\n",
       " '##ο',\n",
       " '##π',\n",
       " '##ρ',\n",
       " '##σ',\n",
       " '##τ',\n",
       " '##υ',\n",
       " '##φ',\n",
       " '##χ',\n",
       " '##ψ',\n",
       " '##ω',\n",
       " '##б',\n",
       " '##г',\n",
       " '##д',\n",
       " '##ж',\n",
       " '##з',\n",
       " '##м',\n",
       " '##п',\n",
       " '##с',\n",
       " '##у',\n",
       " '##ф',\n",
       " '##х',\n",
       " '##ц',\n",
       " '##ч',\n",
       " '##ш',\n",
       " '##щ',\n",
       " '##ъ',\n",
       " '##э',\n",
       " '##ю',\n",
       " '##ђ',\n",
       " '##є',\n",
       " '##і',\n",
       " '##ј',\n",
       " '##љ',\n",
       " '##њ',\n",
       " '##ћ',\n",
       " '##ӏ',\n",
       " '##ա',\n",
       " '##բ',\n",
       " '##գ',\n",
       " '##դ',\n",
       " '##ե',\n",
       " '##թ',\n",
       " '##ի',\n",
       " '##լ',\n",
       " '##կ',\n",
       " '##հ',\n",
       " '##մ',\n",
       " '##յ',\n",
       " '##ն',\n",
       " '##ո',\n",
       " '##պ',\n",
       " '##ս',\n",
       " '##վ',\n",
       " '##տ',\n",
       " '##ր',\n",
       " '##ւ',\n",
       " '##ք',\n",
       " '##־',\n",
       " '##א',\n",
       " '##ב',\n",
       " '##ג',\n",
       " '##ד',\n",
       " '##ו',\n",
       " '##ז',\n",
       " '##ח',\n",
       " '##ט',\n",
       " '##י',\n",
       " '##ך',\n",
       " '##כ',\n",
       " '##ל',\n",
       " '##ם',\n",
       " '##מ',\n",
       " '##ן',\n",
       " '##נ',\n",
       " '##ס',\n",
       " '##ע',\n",
       " '##ף',\n",
       " '##פ',\n",
       " '##ץ',\n",
       " '##צ',\n",
       " '##ק',\n",
       " '##ר',\n",
       " '##ש',\n",
       " '##ת',\n",
       " '##،',\n",
       " '##ء',\n",
       " '##ب',\n",
       " '##ت',\n",
       " '##ث',\n",
       " '##ج',\n",
       " '##ح',\n",
       " '##خ',\n",
       " '##ذ',\n",
       " '##ز',\n",
       " '##س',\n",
       " '##ش',\n",
       " '##ص',\n",
       " '##ض',\n",
       " '##ط',\n",
       " '##ظ',\n",
       " '##ع',\n",
       " '##غ',\n",
       " '##ـ',\n",
       " '##ف',\n",
       " '##ق',\n",
       " '##ك',\n",
       " '##و',\n",
       " '##ى',\n",
       " '##ٹ',\n",
       " '##پ',\n",
       " '##چ',\n",
       " '##ک',\n",
       " '##گ',\n",
       " '##ں',\n",
       " '##ھ',\n",
       " '##ہ',\n",
       " '##ے',\n",
       " '##अ',\n",
       " '##आ',\n",
       " '##उ',\n",
       " '##ए',\n",
       " '##क',\n",
       " '##ख',\n",
       " '##ग',\n",
       " '##च',\n",
       " '##ज',\n",
       " '##ट',\n",
       " '##ड',\n",
       " '##ण',\n",
       " '##त',\n",
       " '##थ',\n",
       " '##द',\n",
       " '##ध',\n",
       " '##न',\n",
       " '##प',\n",
       " '##ब',\n",
       " '##भ',\n",
       " '##म',\n",
       " '##य',\n",
       " '##र',\n",
       " '##ल',\n",
       " '##व',\n",
       " '##श',\n",
       " '##ष',\n",
       " '##स',\n",
       " '##ह',\n",
       " '##ा',\n",
       " '##ि',\n",
       " '##ी',\n",
       " '##ो',\n",
       " '##।',\n",
       " '##॥',\n",
       " '##ং',\n",
       " '##অ',\n",
       " '##আ',\n",
       " '##ই',\n",
       " '##উ',\n",
       " '##এ',\n",
       " '##ও',\n",
       " '##ক',\n",
       " '##খ',\n",
       " '##গ',\n",
       " '##চ',\n",
       " '##ছ',\n",
       " '##জ',\n",
       " '##ট',\n",
       " '##ড',\n",
       " '##ণ',\n",
       " '##ত',\n",
       " '##থ',\n",
       " '##দ',\n",
       " '##ধ',\n",
       " '##ন',\n",
       " '##প',\n",
       " '##ব',\n",
       " '##ভ',\n",
       " '##ম',\n",
       " '##য',\n",
       " '##র',\n",
       " '##ল',\n",
       " '##শ',\n",
       " '##ষ',\n",
       " '##স',\n",
       " '##হ',\n",
       " '##া',\n",
       " '##ি',\n",
       " '##ী',\n",
       " '##ে',\n",
       " '##க',\n",
       " '##ச',\n",
       " '##ட',\n",
       " '##த',\n",
       " '##ந',\n",
       " '##ன',\n",
       " '##ப',\n",
       " '##ம',\n",
       " '##ய',\n",
       " '##ர',\n",
       " '##ல',\n",
       " '##ள',\n",
       " '##வ',\n",
       " '##ா',\n",
       " '##ி',\n",
       " '##ு',\n",
       " '##ே',\n",
       " '##ை',\n",
       " '##ನ',\n",
       " '##ರ',\n",
       " '##ಾ',\n",
       " '##ක',\n",
       " '##ය',\n",
       " '##ර',\n",
       " '##ල',\n",
       " '##ව',\n",
       " '##ා',\n",
       " '##ก',\n",
       " '##ง',\n",
       " '##ต',\n",
       " '##ท',\n",
       " '##น',\n",
       " '##พ',\n",
       " '##ม',\n",
       " '##ย',\n",
       " '##ร',\n",
       " '##ล',\n",
       " '##ว',\n",
       " '##ส',\n",
       " '##อ',\n",
       " '##า',\n",
       " '##เ',\n",
       " '##་',\n",
       " '##།',\n",
       " '##ག',\n",
       " '##ང',\n",
       " '##ད',\n",
       " '##ན',\n",
       " '##པ',\n",
       " '##བ',\n",
       " '##མ',\n",
       " '##འ',\n",
       " '##ར',\n",
       " '##ལ',\n",
       " '##ས',\n",
       " '##မ',\n",
       " '##ა',\n",
       " '##ბ',\n",
       " '##გ',\n",
       " '##დ',\n",
       " '##ე',\n",
       " '##ვ',\n",
       " '##თ',\n",
       " '##ი',\n",
       " '##კ',\n",
       " '##ლ',\n",
       " '##მ',\n",
       " '##ნ',\n",
       " '##ო',\n",
       " '##რ',\n",
       " '##ს',\n",
       " '##ტ',\n",
       " '##უ',\n",
       " '##ᄀ',\n",
       " '##ᄂ',\n",
       " '##ᄃ',\n",
       " '##ᄅ',\n",
       " '##ᄆ',\n",
       " '##ᄇ',\n",
       " '##ᄉ',\n",
       " '##ᄊ',\n",
       " '##ᄋ',\n",
       " '##ᄌ',\n",
       " '##ᄎ',\n",
       " '##ᄏ',\n",
       " '##ᄐ',\n",
       " '##ᄑ',\n",
       " '##ᄒ',\n",
       " '##ᅡ',\n",
       " '##ᅢ',\n",
       " '##ᅥ',\n",
       " '##ᅦ',\n",
       " '##ᅧ',\n",
       " '##ᅩ',\n",
       " '##ᅪ',\n",
       " '##ᅭ',\n",
       " '##ᅮ',\n",
       " '##ᅯ',\n",
       " '##ᅲ',\n",
       " '##ᅳ',\n",
       " '##ᅴ',\n",
       " '##ᅵ',\n",
       " '##ᆨ',\n",
       " '##ᆫ',\n",
       " '##ᆯ',\n",
       " '##ᆷ',\n",
       " '##ᆸ',\n",
       " '##ᆼ',\n",
       " '##ᴬ',\n",
       " '##ᴮ',\n",
       " '##ᴰ',\n",
       " '##ᴵ',\n",
       " '##ᴺ',\n",
       " '##ᵀ',\n",
       " '##ᵃ',\n",
       " '##ᵇ',\n",
       " '##ᵈ',\n",
       " '##ᵉ',\n",
       " '##ᵍ',\n",
       " '##ᵏ',\n",
       " '##ᵐ',\n",
       " '##ᵒ',\n",
       " '##ᵖ',\n",
       " '##ᵗ',\n",
       " '##ᵘ',\n",
       " '##ᵣ',\n",
       " '##ᵤ',\n",
       " '##ᵥ',\n",
       " '##ᶜ',\n",
       " '##ᶠ',\n",
       " '##‐',\n",
       " '##‑',\n",
       " '##‒',\n",
       " '##–',\n",
       " '##—',\n",
       " '##―',\n",
       " '##‖',\n",
       " '##‘',\n",
       " '##’',\n",
       " '##‚',\n",
       " '##“',\n",
       " '##”',\n",
       " '##„',\n",
       " '##†',\n",
       " '##‡',\n",
       " '##•',\n",
       " '##…',\n",
       " '##‰',\n",
       " '##′',\n",
       " '##″',\n",
       " '##›',\n",
       " '##‿',\n",
       " '##⁄',\n",
       " '##⁰',\n",
       " '##ⁱ',\n",
       " '##⁴',\n",
       " '##⁵',\n",
       " '##⁶',\n",
       " '##⁷',\n",
       " '##⁸',\n",
       " '##⁹',\n",
       " '##⁻',\n",
       " '##ⁿ',\n",
       " '##₅',\n",
       " '##₆',\n",
       " '##₇',\n",
       " '##₈',\n",
       " '##₉',\n",
       " '##₊',\n",
       " '##₍',\n",
       " '##₎',\n",
       " '##ₐ',\n",
       " '##ₑ',\n",
       " '##ₒ',\n",
       " '##ₓ',\n",
       " '##ₕ',\n",
       " '##ₖ',\n",
       " '##ₗ',\n",
       " '##ₘ',\n",
       " '##ₚ',\n",
       " '##ₛ',\n",
       " '##ₜ',\n",
       " '##₤',\n",
       " '##₩',\n",
       " '##€',\n",
       " '##₱',\n",
       " '##₹',\n",
       " '##ℓ',\n",
       " '##№',\n",
       " '##ℝ',\n",
       " '##™',\n",
       " '##⅓',\n",
       " '##⅔',\n",
       " '##←',\n",
       " '##↑',\n",
       " '##→',\n",
       " '##↓',\n",
       " '##↔',\n",
       " '##↦',\n",
       " '##⇄',\n",
       " '##⇌',\n",
       " '##⇒',\n",
       " '##∂',\n",
       " '##∅',\n",
       " '##∆',\n",
       " '##∇',\n",
       " '##∈',\n",
       " '##∗',\n",
       " '##∘',\n",
       " '##√',\n",
       " '##∞',\n",
       " '##∧',\n",
       " '##∨',\n",
       " '##∩',\n",
       " '##∪',\n",
       " '##≈',\n",
       " '##≡',\n",
       " '##≤',\n",
       " '##≥',\n",
       " '##⊂',\n",
       " '##⊆',\n",
       " '##⊕',\n",
       " '##⊗',\n",
       " '##⋅',\n",
       " '##─',\n",
       " '##│',\n",
       " '##■',\n",
       " '##▪',\n",
       " '##●',\n",
       " '##★',\n",
       " '##☆',\n",
       " '##☉',\n",
       " '##♠',\n",
       " '##♣',\n",
       " '##♥',\n",
       " '##♦',\n",
       " '##♯',\n",
       " '##⟨',\n",
       " '##⟩',\n",
       " '##ⱼ',\n",
       " '##⺩',\n",
       " '##⺼',\n",
       " '##⽥',\n",
       " '##、',\n",
       " '##。',\n",
       " '##〈',\n",
       " '##〉',\n",
       " '##《',\n",
       " '##》',\n",
       " '##「',\n",
       " '##」',\n",
       " '##『',\n",
       " '##』',\n",
       " '##〜',\n",
       " '##あ',\n",
       " '##い',\n",
       " '##う',\n",
       " '##え',\n",
       " '##お',\n",
       " '##か',\n",
       " '##き',\n",
       " '##く',\n",
       " '##け',\n",
       " '##こ',\n",
       " '##さ',\n",
       " '##し',\n",
       " '##す',\n",
       " '##せ',\n",
       " '##そ',\n",
       " '##た',\n",
       " '##ち',\n",
       " '##っ',\n",
       " '##つ',\n",
       " '##て',\n",
       " '##と',\n",
       " '##な',\n",
       " '##に',\n",
       " '##ぬ',\n",
       " '##ね',\n",
       " '##の',\n",
       " '##は',\n",
       " '##ひ',\n",
       " '##ふ',\n",
       " '##へ',\n",
       " '##ほ',\n",
       " '##ま',\n",
       " '##み',\n",
       " '##む',\n",
       " '##め',\n",
       " '##も',\n",
       " '##や',\n",
       " '##ゆ',\n",
       " '##よ',\n",
       " '##ら',\n",
       " '##り',\n",
       " '##る',\n",
       " '##れ',\n",
       " '##ろ',\n",
       " '##を',\n",
       " '##ん',\n",
       " '##ァ',\n",
       " '##ア',\n",
       " '##ィ',\n",
       " '##イ',\n",
       " '##ウ',\n",
       " '##ェ',\n",
       " '##エ',\n",
       " '##オ',\n",
       " '##カ',\n",
       " '##キ',\n",
       " '##ク',\n",
       " '##ケ',\n",
       " '##コ',\n",
       " '##サ',\n",
       " '##シ',\n",
       " '##ス',\n",
       " '##セ',\n",
       " '##タ',\n",
       " '##チ',\n",
       " '##ッ',\n",
       " '##ツ',\n",
       " '##テ',\n",
       " '##ト',\n",
       " '##ナ',\n",
       " '##ニ',\n",
       " '##ノ',\n",
       " '##ハ',\n",
       " '##ヒ',\n",
       " '##フ',\n",
       " '##ヘ',\n",
       " '##ホ',\n",
       " '##マ',\n",
       " '##ミ',\n",
       " '##ム',\n",
       " '##メ',\n",
       " '##モ',\n",
       " '##ャ',\n",
       " '##ュ',\n",
       " '##ョ',\n",
       " '##ラ',\n",
       " '##リ',\n",
       " '##ル',\n",
       " '##レ',\n",
       " '##ロ',\n",
       " '##ワ',\n",
       " '##ン',\n",
       " '##・',\n",
       " '##ー',\n",
       " '##一',\n",
       " '##三',\n",
       " '##上',\n",
       " '##下',\n",
       " '##不',\n",
       " '##世',\n",
       " '##中',\n",
       " '##主',\n",
       " '##久',\n",
       " '##之',\n",
       " '##也',\n",
       " '##事',\n",
       " '##二',\n",
       " '##五',\n",
       " '##井',\n",
       " '##京',\n",
       " '##人',\n",
       " '##亻',\n",
       " '##仁',\n",
       " '##介',\n",
       " '##代',\n",
       " '##仮',\n",
       " '##伊',\n",
       " '##会',\n",
       " '##佐',\n",
       " '##侍',\n",
       " '##保',\n",
       " '##信',\n",
       " '##健',\n",
       " '##元',\n",
       " '##光',\n",
       " '##八',\n",
       " '##公',\n",
       " '##内',\n",
       " '##出',\n",
       " '##分',\n",
       " '##前',\n",
       " '##劉',\n",
       " '##力',\n",
       " '##加',\n",
       " '##勝',\n",
       " '##北',\n",
       " '##区',\n",
       " '##十',\n",
       " '##千',\n",
       " '##南',\n",
       " '##博',\n",
       " '##原',\n",
       " '##口',\n",
       " '##古',\n",
       " '##史',\n",
       " '##司',\n",
       " '##合',\n",
       " '##吉',\n",
       " '##同',\n",
       " '##名',\n",
       " '##和',\n",
       " '##囗',\n",
       " '##四',\n",
       " '##国',\n",
       " '##國',\n",
       " '##土',\n",
       " '##地',\n",
       " '##坂',\n",
       " '##城',\n",
       " '##堂',\n",
       " '##場',\n",
       " '##士',\n",
       " '##夏',\n",
       " '##外',\n",
       " '##大',\n",
       " '##天',\n",
       " '##太',\n",
       " '##夫',\n",
       " '##奈',\n",
       " '##女',\n",
       " '##子',\n",
       " '##学',\n",
       " '##宀',\n",
       " '##宇',\n",
       " '##安',\n",
       " '##宗',\n",
       " '##定',\n",
       " '##宣',\n",
       " '##宮',\n",
       " '##家',\n",
       " '##宿',\n",
       " '##寺',\n",
       " '##將',\n",
       " '##小',\n",
       " '##尚',\n",
       " '##山',\n",
       " '##岡',\n",
       " '##島',\n",
       " '##崎',\n",
       " '##川',\n",
       " '##州',\n",
       " '##巿',\n",
       " '##帝',\n",
       " '##平',\n",
       " '##年',\n",
       " '##幸',\n",
       " '##广',\n",
       " '##弘',\n",
       " '##張',\n",
       " '##彳',\n",
       " '##後',\n",
       " '##御',\n",
       " '##德',\n",
       " '##心',\n",
       " '##忄',\n",
       " '##志',\n",
       " '##忠',\n",
       " '##愛',\n",
       " '##成',\n",
       " '##我',\n",
       " '##戦',\n",
       " '##戸',\n",
       " '##手',\n",
       " '##扌',\n",
       " '##政',\n",
       " '##文',\n",
       " '##新',\n",
       " '##方',\n",
       " '##日',\n",
       " '##明',\n",
       " '##星',\n",
       " '##春',\n",
       " '##昭',\n",
       " '##智',\n",
       " '##曲',\n",
       " '##書',\n",
       " '##月',\n",
       " '##有',\n",
       " '##朝',\n",
       " '##木',\n",
       " '##本',\n",
       " '##李',\n",
       " '##村',\n",
       " '##東',\n",
       " '##松',\n",
       " '##林',\n",
       " '##森',\n",
       " '##楊',\n",
       " '##樹',\n",
       " '##橋',\n",
       " '##歌',\n",
       " '##止',\n",
       " '##正',\n",
       " '##武',\n",
       " '##比',\n",
       " '##氏',\n",
       " '##民',\n",
       " '##水',\n",
       " '##氵',\n",
       " '##氷',\n",
       " '##永',\n",
       " '##江',\n",
       " '##沢',\n",
       " '##河',\n",
       " '##治',\n",
       " '##法',\n",
       " '##海',\n",
       " '##清',\n",
       " '##漢',\n",
       " '##瀬',\n",
       " '##火',\n",
       " '##版',\n",
       " '##犬',\n",
       " '##王',\n",
       " '##生',\n",
       " '##田',\n",
       " '##男',\n",
       " '##疒',\n",
       " '##発',\n",
       " '##白',\n",
       " '##的',\n",
       " '##皇',\n",
       " '##目',\n",
       " '##相',\n",
       " '##省',\n",
       " '##真',\n",
       " '##石',\n",
       " '##示',\n",
       " '##社',\n",
       " '##神',\n",
       " '##福',\n",
       " '##禾',\n",
       " '##秀',\n",
       " '##秋',\n",
       " '##空',\n",
       " '##立',\n",
       " '##章',\n",
       " '##竹',\n",
       " '##糹',\n",
       " '##美',\n",
       " '##義',\n",
       " '##耳',\n",
       " '##良',\n",
       " '##艹',\n",
       " '##花',\n",
       " '##英',\n",
       " '##華',\n",
       " '##葉',\n",
       " '##藤',\n",
       " '##行',\n",
       " '##街',\n",
       " '##西',\n",
       " '##見',\n",
       " '##訁',\n",
       " '##語',\n",
       " '##谷',\n",
       " '##貝',\n",
       " '##貴',\n",
       " '##車',\n",
       " '##軍',\n",
       " '##辶',\n",
       " '##道',\n",
       " '##郎',\n",
       " '##郡',\n",
       " '##部',\n",
       " '##都',\n",
       " '##里',\n",
       " '##野',\n",
       " '##金',\n",
       " '##鈴',\n",
       " '##镇',\n",
       " '##長',\n",
       " '##門',\n",
       " '##間',\n",
       " '##阝',\n",
       " '##阿',\n",
       " '##陳',\n",
       " '##陽',\n",
       " '##雄',\n",
       " '##青',\n",
       " '##面',\n",
       " '##風',\n",
       " '##食',\n",
       " '##香',\n",
       " '##馬',\n",
       " '##高',\n",
       " '##龍',\n",
       " '##龸',\n",
       " '##ﬁ',\n",
       " '##ﬂ',\n",
       " '##！',\n",
       " '##（',\n",
       " '##）',\n",
       " '##，',\n",
       " '##－',\n",
       " '##．',\n",
       " '##／',\n",
       " '##：',\n",
       " '##？',\n",
       " '##～']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## exploring the vocabulary of the BERT tokenizer\n",
    "\n",
    "## [0] - [PAD]\n",
    "## [100] - [UNK]\n",
    "## [101] - [CLS]\n",
    "## [102] - [SEP]\n",
    "## [104] - [MASK]\n",
    "## [999:1996] are starting characters ! ... ~\n",
    "## [1996:29612] are words\n",
    "## [29612:] are subwords\n",
    "\n",
    "\n",
    "list(tokenizer.vocab.keys())[29612:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 101\n",
      "i 1045\n",
      "wish 4299\n",
      "i 1045\n",
      "can 2064\n",
      "be 2022\n",
      "more 2062\n",
      "productive 13318\n",
      "and 1998\n",
      "be 2022\n",
      "less 2625\n",
      "tempted 16312\n",
      "to 2000\n",
      "play 2377\n",
      "video 2678\n",
      "games 2399\n",
      "[SEP] 102\n"
     ]
    }
   ],
   "source": [
    "## map words to indices\n",
    "\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "for token, id in zip(tokenized_text, token_ids):\n",
    "    print(token, id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 101 1\n",
      "i 1045 1\n",
      "wish 4299 1\n",
      "i 1045 1\n",
      "can 2064 1\n",
      "be 2022 1\n",
      "more 2062 1\n",
      "productive 13318 1\n",
      "and 1998 1\n",
      "be 2022 1\n",
      "less 2625 1\n",
      "tempted 16312 1\n",
      "to 2000 1\n",
      "play 2377 1\n",
      "video 2678 1\n",
      "games 2399 1\n",
      "[SEP] 102 1\n"
     ]
    }
   ],
   "source": [
    "## add sentence ids to each token\n",
    "\n",
    "sentence_ids = [1] * len(token_ids)\n",
    "\n",
    "for token, token_id, sentence_id in zip(tokenized_text, token_ids, sentence_ids):\n",
    "    print(token, token_id, sentence_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1045,  4299,  1045,  2064,  2022,  2062, 13318,  1998,  2022,\n",
      "          2625, 16312,  2000,  2377,  2678,  2399,   102]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "## We need to turn token id and segment id lists into tensors\n",
    "\n",
    "tokens_tensor = torch.tensor([token_ids])\n",
    "\n",
    "segment_tensor = torch.tensor([sentence_ids])\n",
    "\n",
    "print(tokens_tensor)\n",
    "\n",
    "print(segment_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load the BERT pretrained model\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0 shape: torch.Size([1, 17, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.1686, -0.2858, -0.3261])\n",
      "\t\ti: tensor([-3.4026e-04,  5.3974e-01, -2.8805e-01])\n",
      "\t\twish: tensor([-0.4765, -0.4256, -0.0395])\n",
      "\t\ti: tensor([-0.2035,  0.3146, -0.3127])\n",
      "\t\tcan: tensor([ 1.0617, -0.8491,  0.7407])\n",
      "\t\tbe: tensor([ 0.5315, -0.0140, -0.7170])\n",
      "\t\tmore: tensor([ 0.8776,  0.2554, -0.4276])\n",
      "\t\tproductive: tensor([-0.1674,  1.1317, -0.7485])\n",
      "\t\tand: tensor([-0.1952,  0.1577, -0.1134])\n",
      "\t\tbe: tensor([ 0.7034,  0.2527, -0.6757])\n",
      "\t\tless: tensor([ 0.2373,  0.7896, -0.0711])\n",
      "\t\ttempted: tensor([0.2990, 0.1633, 0.6750])\n",
      "\t\tto: tensor([0.1360, 0.5616, 0.0765])\n",
      "\t\tplay: tensor([-1.0072,  0.5862, -0.2050])\n",
      "\t\tvideo: tensor([ 0.3764,  0.8183, -0.1226])\n",
      "\t\tgames: tensor([-0.1782,  1.0912,  0.5143])\n",
      "\t\t[SEP]: tensor([-0.5870,  0.2658,  0.0439])\n",
      "layer: 1 shape: torch.Size([1, 17, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.0586,  0.0456, -0.0594])\n",
      "\t\ti: tensor([ 0.4496,  0.4641, -0.3497])\n",
      "\t\twish: tensor([-0.4856, -0.2916,  0.1197])\n",
      "\t\ti: tensor([ 0.0045,  0.1801, -0.3104])\n",
      "\t\tcan: tensor([ 0.8733, -0.9297,  0.6224])\n",
      "\t\tbe: tensor([ 0.7701,  0.0618, -1.2215])\n",
      "\t\tmore: tensor([ 0.8337,  0.0157, -0.5403])\n",
      "\t\tproductive: tensor([-0.1278,  1.0617, -1.1299])\n",
      "\t\tand: tensor([-0.4025, -0.1427, -0.1071])\n",
      "\t\tbe: tensor([ 0.9304,  0.3479, -0.7302])\n",
      "\t\tless: tensor([ 0.2717,  0.8372, -0.2012])\n",
      "\t\ttempted: tensor([ 0.5179, -0.6651,  0.6830])\n",
      "\t\tto: tensor([0.1895, 0.3726, 0.5545])\n",
      "\t\tplay: tensor([-0.9212,  0.4587, -0.1340])\n",
      "\t\tvideo: tensor([0.5587, 0.9512, 0.2481])\n",
      "\t\tgames: tensor([-0.0284,  0.6668,  0.3041])\n",
      "\t\t[SEP]: tensor([-0.2881,  0.3954,  0.1510])\n",
      "layer: 2 shape: torch.Size([1, 17, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([-0.0883, -0.1949, -0.2573])\n",
      "\t\ti: tensor([0.4481, 0.1847, 0.0023])\n",
      "\t\twish: tensor([-0.2679, -0.1170, -0.0075])\n",
      "\t\ti: tensor([-0.1389, -0.1052, -0.2608])\n",
      "\t\tcan: tensor([ 0.6477, -0.7155,  0.8740])\n",
      "\t\tbe: tensor([ 0.9626, -0.1902, -0.5813])\n",
      "\t\tmore: tensor([ 1.0586,  0.2459, -0.0748])\n",
      "\t\tproductive: tensor([-0.4392,  1.6465, -1.3013])\n",
      "\t\tand: tensor([-0.3326, -0.2792,  0.0195])\n",
      "\t\tbe: tensor([ 1.1035, -0.1247, -0.2951])\n",
      "\t\tless: tensor([ 0.4507,  0.9192, -0.0919])\n",
      "\t\ttempted: tensor([ 0.6467, -0.9634,  0.7505])\n",
      "\t\tto: tensor([0.2223, 0.2294, 1.0107])\n",
      "\t\tplay: tensor([-0.3959,  0.8840, -0.3528])\n",
      "\t\tvideo: tensor([ 0.6484,  1.5421, -0.2764])\n",
      "\t\tgames: tensor([-0.2906,  0.7167,  0.1441])\n",
      "\t\t[SEP]: tensor([-0.2184,  0.2242,  0.2778])\n",
      "layer: 3 shape: torch.Size([1, 17, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([-0.0137, -0.2872, -0.0914])\n",
      "\t\ti: tensor([ 0.9096, -0.1032,  0.2807])\n",
      "\t\twish: tensor([ 0.1227, -0.1298, -0.0836])\n",
      "\t\ti: tensor([ 0.4551, -0.3824, -0.2264])\n",
      "\t\tcan: tensor([ 0.5862, -0.6436,  0.9721])\n",
      "\t\tbe: tensor([ 0.9927, -0.2998, -0.4498])\n",
      "\t\tmore: tensor([ 1.1157, -0.0842, -0.5131])\n",
      "\t\tproductive: tensor([-0.5598,  1.4125, -1.4320])\n",
      "\t\tand: tensor([-0.3587, -0.2261,  0.2290])\n",
      "\t\tbe: tensor([ 1.2451, -0.1873, -0.5213])\n",
      "\t\tless: tensor([ 0.5483,  0.9476, -0.4022])\n",
      "\t\ttempted: tensor([ 0.6520, -1.1844,  0.8443])\n",
      "\t\tto: tensor([0.3473, 0.3274, 1.1524])\n",
      "\t\tplay: tensor([ 0.1630,  1.4294, -0.0656])\n",
      "\t\tvideo: tensor([0.5143, 1.0853, 0.6761])\n",
      "\t\tgames: tensor([-0.2129,  1.1581, -0.0911])\n",
      "\t\t[SEP]: tensor([-0.0535, -0.0729,  0.1572])\n",
      "layer: 4 shape: torch.Size([1, 17, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.2725, -0.5611, -0.5663])\n",
      "\t\ti: tensor([ 1.2519, -0.5165, -0.0783])\n",
      "\t\twish: tensor([ 0.5412, -0.5380, -0.1793])\n",
      "\t\ti: tensor([ 0.2432, -0.7561, -0.4903])\n",
      "\t\tcan: tensor([ 0.4082, -0.5997,  0.7315])\n",
      "\t\tbe: tensor([ 0.7507, -0.5656, -0.4849])\n",
      "\t\tmore: tensor([ 0.7126,  0.0295, -0.4538])\n",
      "\t\tproductive: tensor([-0.4643,  0.9424, -1.6935])\n",
      "\t\tand: tensor([-0.5513, -0.3239,  0.3472])\n",
      "\t\tbe: tensor([ 0.7861, -0.0143, -0.4778])\n",
      "\t\tless: tensor([ 0.1316,  0.9087, -0.0972])\n",
      "\t\ttempted: tensor([ 0.5728, -1.1127,  0.7154])\n",
      "\t\tto: tensor([0.4066, 0.5243, 0.9830])\n",
      "\t\tplay: tensor([0.4314, 1.8734, 0.4268])\n",
      "\t\tvideo: tensor([0.3543, 0.7687, 0.6524])\n",
      "\t\tgames: tensor([ 0.0091,  0.9673, -0.1320])\n",
      "\t\t[SEP]: tensor([-0.0201, -0.0554,  0.0190])\n",
      "layer: 5 shape: torch.Size([1, 17, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.1220, -0.6545, -0.3439])\n",
      "\t\ti: tensor([ 1.0832, -0.4355, -0.2485])\n",
      "\t\twish: tensor([ 0.3933, -0.0138, -0.3780])\n",
      "\t\ti: tensor([ 0.1729, -0.3470, -0.3620])\n",
      "\t\tcan: tensor([ 0.1535, -0.3730,  0.3899])\n",
      "\t\tbe: tensor([ 0.5450, -0.9268, -0.3350])\n",
      "\t\tmore: tensor([ 0.0291,  0.0925, -0.3844])\n",
      "\t\tproductive: tensor([-0.4947,  0.9397, -1.5286])\n",
      "\t\tand: tensor([-0.3585, -0.2620,  0.1871])\n",
      "\t\tbe: tensor([ 0.8952,  0.1083, -0.2409])\n",
      "\t\tless: tensor([0.2482, 1.0812, 0.0219])\n",
      "\t\ttempted: tensor([ 0.6793, -0.8378,  0.3804])\n",
      "\t\tto: tensor([0.5426, 0.2432, 0.4931])\n",
      "\t\tplay: tensor([1.0527, 1.7617, 0.6182])\n",
      "\t\tvideo: tensor([0.5170, 0.4569, 0.7845])\n",
      "\t\tgames: tensor([ 0.3732,  0.7331, -0.1668])\n",
      "\t\t[SEP]: tensor([-0.0216, -0.0317,  0.0211])\n",
      "layer: 6 shape: torch.Size([1, 17, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.3296, -0.7476, -0.4637])\n",
      "\t\ti: tensor([ 0.9951, -0.0483, -0.6969])\n",
      "\t\twish: tensor([ 0.2838,  0.0222, -0.6388])\n",
      "\t\ti: tensor([ 0.3485, -0.2249, -0.7694])\n",
      "\t\tcan: tensor([-0.1829, -0.2454, -0.2829])\n",
      "\t\tbe: tensor([-0.1353, -0.6299, -0.9820])\n",
      "\t\tmore: tensor([-0.2786, -0.0603, -0.8335])\n",
      "\t\tproductive: tensor([-0.4403,  1.1403, -1.1668])\n",
      "\t\tand: tensor([-0.3569, -0.8310, -0.0573])\n",
      "\t\tbe: tensor([ 0.5159, -0.2511, -0.5844])\n",
      "\t\tless: tensor([ 0.4014,  0.9269, -0.0515])\n",
      "\t\ttempted: tensor([ 0.5717, -0.9810,  0.5500])\n",
      "\t\tto: tensor([0.5754, 0.7287, 0.3169])\n",
      "\t\tplay: tensor([0.7861, 1.9052, 0.4698])\n",
      "\t\tvideo: tensor([0.5844, 0.5483, 0.5358])\n",
      "\t\tgames: tensor([ 0.4792,  1.3415, -0.2671])\n",
      "\t\t[SEP]: tensor([ 0.0178, -0.0344, -0.0136])\n",
      "layer: 7 shape: torch.Size([1, 17, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.6800, -0.7239, -0.3604])\n",
      "\t\ti: tensor([ 0.8449, -0.2609, -0.7105])\n",
      "\t\twish: tensor([ 0.2922,  0.3742, -0.1363])\n",
      "\t\ti: tensor([ 0.3005, -0.4632, -0.7280])\n",
      "\t\tcan: tensor([-0.7017, -1.1398,  0.1155])\n",
      "\t\tbe: tensor([-0.1711, -0.7805, -0.9801])\n",
      "\t\tmore: tensor([ 0.1113, -0.4735, -0.7736])\n",
      "\t\tproductive: tensor([-0.4569,  1.0571, -1.3467])\n",
      "\t\tand: tensor([-0.4206, -0.7916, -0.5452])\n",
      "\t\tbe: tensor([ 0.4468, -0.2023, -0.2207])\n",
      "\t\tless: tensor([0.0386, 0.1761, 0.0966])\n",
      "\t\ttempted: tensor([ 0.6421, -0.9284,  0.2896])\n",
      "\t\tto: tensor([0.6000, 0.3410, 0.4027])\n",
      "\t\tplay: tensor([0.8559, 1.8718, 0.8555])\n",
      "\t\tvideo: tensor([0.7029, 0.4179, 0.5363])\n",
      "\t\tgames: tensor([ 0.5359,  0.7211, -0.1921])\n",
      "\t\t[SEP]: tensor([ 0.0149, -0.0166, -0.0039])\n",
      "layer: 8 shape: torch.Size([1, 17, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.7945, -0.4241, -0.7279])\n",
      "\t\ti: tensor([ 0.6069, -0.0197, -0.6797])\n",
      "\t\twish: tensor([ 0.5674,  0.2203, -0.0441])\n",
      "\t\ti: tensor([-0.0913, -0.4103, -0.7730])\n",
      "\t\tcan: tensor([-0.5129, -1.3765, -0.6068])\n",
      "\t\tbe: tensor([-0.5579, -0.7212, -1.0932])\n",
      "\t\tmore: tensor([-0.5660, -0.6150, -0.8478])\n",
      "\t\tproductive: tensor([-0.4020,  0.6983, -1.1183])\n",
      "\t\tand: tensor([-0.5182, -1.1126, -0.6481])\n",
      "\t\tbe: tensor([-0.1147, -0.2665, -0.3709])\n",
      "\t\tless: tensor([-0.4772, -0.0407,  0.0058])\n",
      "\t\ttempted: tensor([ 0.3727, -1.0312, -0.0023])\n",
      "\t\tto: tensor([ 0.6872, -0.0449,  0.0293])\n",
      "\t\tplay: tensor([0.6633, 1.3893, 0.5809])\n",
      "\t\tvideo: tensor([0.4682, 0.4912, 0.1685])\n",
      "\t\tgames: tensor([ 0.3253,  0.8288, -0.7192])\n",
      "\t\t[SEP]: tensor([ 0.0184, -0.0153,  0.0217])\n",
      "layer: 9 shape: torch.Size([1, 17, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.6655, -0.0539, -0.5147])\n",
      "\t\ti: tensor([ 0.4635,  0.4612, -0.5004])\n",
      "\t\twish: tensor([0.5593, 0.4350, 0.0096])\n",
      "\t\ti: tensor([ 0.3951, -0.0644, -0.5455])\n",
      "\t\tcan: tensor([-0.1507, -1.3191, -0.6584])\n",
      "\t\tbe: tensor([-0.4502, -0.4579, -0.8062])\n",
      "\t\tmore: tensor([-0.3955, -0.5503, -0.7061])\n",
      "\t\tproductive: tensor([-0.0335,  0.7467, -0.7879])\n",
      "\t\tand: tensor([-0.2124, -1.0697, -0.5218])\n",
      "\t\tbe: tensor([ 0.3877, -0.1206, -0.0318])\n",
      "\t\tless: tensor([-0.1564, -0.2123, -0.1397])\n",
      "\t\ttempted: tensor([ 0.5069, -0.7653,  0.0820])\n",
      "\t\tto: tensor([1.3509, 0.0366, 0.3092])\n",
      "\t\tplay: tensor([0.7981, 1.1365, 0.6838])\n",
      "\t\tvideo: tensor([0.2333, 0.4719, 0.2029])\n",
      "\t\tgames: tensor([ 0.2933,  0.9769, -0.5020])\n",
      "\t\t[SEP]: tensor([0.0128, 0.0118, 0.0197])\n",
      "layer: 10 shape: torch.Size([1, 17, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.5923, -0.1704, -0.3223])\n",
      "\t\ti: tensor([ 0.7079,  0.3955, -0.2126])\n",
      "\t\twish: tensor([ 0.4301,  0.7695, -0.1025])\n",
      "\t\ti: tensor([ 0.4989,  0.1256, -0.3386])\n",
      "\t\tcan: tensor([-0.0379, -0.7376, -0.2431])\n",
      "\t\tbe: tensor([-0.0914, -0.3244, -0.7668])\n",
      "\t\tmore: tensor([-0.4063, -0.5898, -1.3507])\n",
      "\t\tproductive: tensor([ 0.2481,  0.5837, -0.7044])\n",
      "\t\tand: tensor([ 0.0771, -0.7361, -0.4174])\n",
      "\t\tbe: tensor([ 0.0153, -0.0024, -0.4032])\n",
      "\t\tless: tensor([-0.5269, -0.1786, -0.5957])\n",
      "\t\ttempted: tensor([ 0.5895, -0.9644,  0.2069])\n",
      "\t\tto: tensor([1.0113, 0.1729, 0.3279])\n",
      "\t\tplay: tensor([0.9907, 1.1730, 0.4475])\n",
      "\t\tvideo: tensor([ 0.4718,  0.1299, -0.2049])\n",
      "\t\tgames: tensor([ 0.3242,  0.7390, -0.3977])\n",
      "\t\t[SEP]: tensor([ 0.0302,  0.0300, -0.0752])\n",
      "layer: 11 shape: torch.Size([1, 17, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.5903, -0.1216,  0.0312])\n",
      "\t\ti: tensor([ 0.7375,  0.3062, -0.1067])\n",
      "\t\twish: tensor([0.6890, 0.9030, 0.0297])\n",
      "\t\ti: tensor([ 0.6102,  0.1289, -0.1585])\n",
      "\t\tcan: tensor([ 0.2531, -0.5014,  0.0535])\n",
      "\t\tbe: tensor([ 0.0169, -0.3018, -0.5922])\n",
      "\t\tmore: tensor([-0.2687, -0.8539, -1.3376])\n",
      "\t\tproductive: tensor([ 0.4089,  0.2946, -0.4792])\n",
      "\t\tand: tensor([-0.0100, -0.6797, -0.3694])\n",
      "\t\tbe: tensor([ 0.1445, -0.3146, -0.3001])\n",
      "\t\tless: tensor([-0.5376, -0.7219, -0.8711])\n",
      "\t\ttempted: tensor([ 0.5234, -0.7896,  0.0619])\n",
      "\t\tto: tensor([0.9374, 0.3231, 0.1637])\n",
      "\t\tplay: tensor([0.8685, 1.3586, 0.4866])\n",
      "\t\tvideo: tensor([ 0.5122,  0.1675, -0.1106])\n",
      "\t\tgames: tensor([ 0.3595,  0.7290, -0.0168])\n",
      "\t\t[SEP]: tensor([ 0.0362,  0.0207, -0.0264])\n",
      "layer: 12 shape: torch.Size([1, 17, 768])\n",
      "\t length of a sentence is 1\n",
      "\t\t[CLS]: tensor([ 0.3246,  0.3165, -0.1085])\n",
      "\t\ti: tensor([ 0.2805,  0.3541, -0.3777])\n",
      "\t\twish: tensor([0.2140, 0.5532, 0.2470])\n",
      "\t\ti: tensor([0.3993, 0.1602, 0.0859])\n",
      "\t\tcan: tensor([ 0.3594, -0.2082,  0.2150])\n",
      "\t\tbe: tensor([ 0.0823, -0.2367, -0.3097])\n",
      "\t\tmore: tensor([-0.2828, -0.6498, -0.3860])\n",
      "\t\tproductive: tensor([ 0.2899,  0.1550, -0.3456])\n",
      "\t\tand: tensor([-0.3852, -0.0556, -0.4581])\n",
      "\t\tbe: tensor([ 0.0265, -0.1905, -0.2290])\n",
      "\t\tless: tensor([-0.4236, -0.4347, -0.4813])\n",
      "\t\ttempted: tensor([ 0.2982, -0.3796,  0.1680])\n",
      "\t\tto: tensor([ 0.4241,  0.2798, -0.2184])\n",
      "\t\tplay: tensor([1.0557, 0.8643, 0.2405])\n",
      "\t\tvideo: tensor([ 0.5695,  0.4635, -0.0652])\n",
      "\t\tgames: tensor([ 0.6511,  0.3822, -0.2510])\n",
      "\t\t[SEP]: tensor([ 0.6148,  0.2484, -0.2419])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segment_tensor)\n",
    "\n",
    "    ## output = last layer hidden state, pooler output, hidden state output by layer\n",
    "    hidden_states = outputs[2] # last layer\n",
    "\n",
    "    layer_id = 0\n",
    "\n",
    "    for layer in hidden_states:\n",
    "\n",
    "        print(f\"layer: {layer_id} shape: {layer.shape}\")\n",
    "\n",
    "        print(f\"\\t length of a sentence is {len(layer)}\")\n",
    "\n",
    "        sentence_hidden_layer = layer[0]\n",
    "\n",
    "        for word, output in zip(tokenized_text, sentence_hidden_layer):\n",
    "            print(f\"\\t\\t{word}: {output[:3]}\")\n",
    "\n",
    "        layer_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t[CLS]: tensor([ 0.3246,  0.3165, -0.1085])\n",
      "\t\ti: tensor([ 0.2805,  0.3541, -0.3777])\n",
      "\t\twish: tensor([0.2140, 0.5532, 0.2470])\n",
      "\t\ti: tensor([0.3993, 0.1602, 0.0859])\n",
      "\t\tcan: tensor([ 0.3594, -0.2082,  0.2150])\n",
      "\t\tbe: tensor([ 0.0823, -0.2367, -0.3097])\n",
      "\t\tmore: tensor([-0.2828, -0.6498, -0.3860])\n",
      "\t\tproductive: tensor([ 0.2899,  0.1550, -0.3456])\n",
      "\t\tand: tensor([-0.3852, -0.0556, -0.4581])\n",
      "\t\tbe: tensor([ 0.0265, -0.1905, -0.2290])\n",
      "\t\tless: tensor([-0.4236, -0.4347, -0.4813])\n",
      "\t\ttempted: tensor([ 0.2982, -0.3796,  0.1680])\n",
      "\t\tto: tensor([ 0.4241,  0.2798, -0.2184])\n",
      "\t\tplay: tensor([1.0557, 0.8643, 0.2405])\n",
      "\t\tvideo: tensor([ 0.5695,  0.4635, -0.0652])\n",
      "\t\tgames: tensor([ 0.6511,  0.3822, -0.2510])\n",
      "\t\t[SEP]: tensor([ 0.6148,  0.2484, -0.2419])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segment_tensor)\n",
    "\n",
    "    hidden_states = outputs[2][-1]\n",
    "\n",
    "    for word, output in zip(tokenized_text, sentence_hidden_layer):\n",
    "        print(f\"\\t\\t{word}: {output[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 13 shape: torch.Size([1, 26, 768])\n"
     ]
    }
   ],
   "source": [
    "## use BERT to evaluate our input\n",
    "\n",
    "## [POINT] torch.no_grad will tell pytorch to not make the computing graph on the forward pass.\n",
    "## the forward pass is used during backprop, but since we only need the encoder's output states, we don't need the graph\n",
    "\n",
    "## [POINT] BaseModelOutput (base model's output) returns [0] Last layer's hidden layer [1] hidden layer [2] attentions\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, segment_tensor)\n",
    "\n",
    "    hidden_states = outputs[2][-1]\n",
    "\n",
    "    print(f\"layer: {layer_id} shape: {layer.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
